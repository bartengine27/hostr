{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<ul> <li>Introduction</li> <li>Repository Introduction<ul> <li>Purpose of This Repository</li> <li>Environment Roles Defined</li> </ul> </li> <li>Setup Instructions<ul> <li>Prerequisites</li> <li>Running the Playbooks</li> <li>Expected Outcome</li> <li>Troubleshooting</li> </ul> </li> <li>Contributing</li> <li>Detailed Prerequisites<ul> <li>Proxmox prerequisites</li> <li>Hyper-V install</li> <li>Bare metal install</li> <li>Promox API user</li> <li>Ansible prerequisites</li> <li>Secrets prerequisites</li> <li>Ansible role settings</li> <li>Git/SSH prerequisites</li> <li>Proxmox Firewall Preparation</li> <li>Bridges</li> </ul> </li> <li>Proxmox<ul> <li>Proxmox VM setup</li> <li>Uploading Proxmox VM(s) templates</li> <li>Installing Proxmox VM(s)</li> <li>Addding Proxmox VM(s) to known_hosts</li> <li>Stopping Proxmox VM(s)</li> <li>Starting Proxmox VM(s)</li> <li>Removing Proxmox VM(s)</li> </ul> </li> <li>REDIS<ul> <li>Install REDIS</li> </ul> </li> <li>Authentication Server<ul> <li>Install Authentication Server</li> </ul> </li> <li>REST API<ul> <li>Install REST API</li> </ul> </li> <li>Proxy<ul> <li>Install NGINX Proxy</li> </ul> </li> <li>Database<ul> <li>Install SQL Server</li> </ul> </li> <li>Web server<ul> <li>Install the Blazor Application</li> </ul> </li> <li>Application Performance Monitoring<ul> <li>Prometheus server</li> <li>Install Prometheus server</li> <li>Collector</li> <li>Install InfluxDB</li> <li>Grafana server</li> <li>Install Grafana server</li> </ul> </li> <li>Certificate Authority<ul> <li>Install Certificate Authority</li> <li>Generate Client Certificates</li> </ul> </li> <li>HTTPS Offloading<ul> <li>NGINX</li> <li>Log format</li> <li>Load balancing</li> <li>HTTPS configuration</li> <li>Access logging</li> <li>Location block and proxy settings</li> <li>Optional websocket support</li> </ul> </li> <li>References</li> <li>TODO</li> </ul>"},{"location":"#repository-introduction","title":"Repository Introduction","text":"<p>Welcome to our GitHub repository where you'll find Ansible scripts written to facilitate the development of web applications by mirroring a typical production environment. This approach might address common challenges where hardware or time constraints make it difficult to set up a test or acceptance environment that accurately reflects various production settings.</p>"},{"location":"#purpose-of-this-repository","title":"Purpose of This Repository","text":"<p>This repository hosts Ansible scripts that allow you to deploy a complex web application environment either locally on your development machine, on-premises hardware, or in a cloud setup. By leveraging these scripts, developers may accelerate the setup process and ensure consistency across different environments, from development to production.</p>"},{"location":"#environment-roles-defined","title":"Environment Roles Defined","text":"<p>The provided Ansible scripts are prepared to configure the following roles essential for a full-featured web application infrastructure:</p> <ul> <li>Web Server: Hosts the web application interface.</li> <li>Cache: Improves response time and reduces load on the database by storing recently accessed data.</li> <li>Database: Manages data storage and retrieval.</li> <li>Identity Server: Handles authentication and authorization services.</li> <li>REST API endpoints: Facilitates communication between the client apps and server.</li> <li>Load Balancer: Distributes incoming network traffic across multiple servers.</li> <li>Certificate Authority: Issues SSL/TLS certificates for secured communications.</li> <li>HTTPS offloading: Relieves backend application servers of the processing load associated with encrypting and decrypting public HTTPS traffic.</li> <li>Prometheus monitoring: Collects and stores metrics as time-series data.</li> <li>Grafana dashboards: Provides visualization of the metrics collected by Prometheus.</li> </ul>"},{"location":"#setup-instructions","title":"Setup Instructions","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<p>Before running the Ansible playbooks, ensure that you have the following:</p> <ul> <li>Ansible installed on your local machine or a control node.</li> <li>Sufficient privileges to manage the systems designated in your inventory.</li> <li>Machines (VMs, containers, or bare metal) defined in your hosts file and accessible over your network.</li> </ul>"},{"location":"#running-the-playbooks","title":"Running the Playbooks","text":"<p>To deploy the roles listed above, follow these steps:</p> <ol> <li> <p>Navigate to your local clone of this repository:    Open a terminal and ensure you are in the repository directory.</p> </li> <li> <p>Setup your inventory:    Ensure that your <code>hosts</code> file in the <code>./playbooks</code> directory is updated to reflect the infrastructure where you want to deploy the roles.</p> </li> <li> <p>Configure role variables    Default variables are provided for all roles in <code>role_folder/defaults/vars.yml</code> and initialized by reading environment variables. Example environment variables are available.</p> </li> <li> <p>Execute the Ansible Playbook:    Run the following command to start the deployment process. This command will prompt you for your sudo password due to the <code>-K</code> flag and provide detailed output with the <code>-vvv</code> verbosity level.</p> </li> </ol> <pre><code>ansible-playbook site.yml -K -i hosts -vvv\n</code></pre>"},{"location":"#expected-outcome","title":"Expected Outcome","text":"<p>After executing the playbook, all specified roles should be successfully configured and running across your specified machines. You can verify the status of each role through the corresponding management interfaces or by checking the services directly on each machine.</p>"},{"location":"#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during the setup, consider the following troubleshooting steps:</p> <ul> <li>Verify network connectivity between your control node and the target machines.</li> <li>Check the permissions and authentication details for the accounts used.</li> <li>Review the verbose output provided by the Ansible execution for errors or warnings.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions to improve the scripts or documentation. Please feel free to fork the repository, make your changes, and submit a pull request.</p>"},{"location":"#detailed-prerequisites","title":"Detailed Prerequisites","text":""},{"location":"#proxmox-prerequisites","title":"Proxmox prerequisites","text":"<p>If you'd like to deploy on Proxmox, first install Proxmox on a host. Your host is a bare metal machine or a virtual machine with nesting enabled. Qemu For instance, allows to nest VMs. TBC </p> <p>Depending on your particular installation and/or test requirements, it may be relevant to configure an DNS server during the Proxmox installation: this will ensure that the machine running Proxmox (bare metal or VM) and the VMs running on Proxmox all are in the same subnet.</p> <p>In this document/config it is assumed that you will use the following domain(s):</p> <pre><code># the ip address of the machine that (will) run(s) your web site, for example the ip address of proxmox vm 100\n192.168.1.60 cars.be\n</code></pre> <p>Add the entries above in the hosts file on</p> <ul> <li>the machine running the Ansible installation scripts</li> <li>the clients you use to visit the web site</li> </ul> <p>If all has gone well, you should have (after running the Ansible scripts) a setup like:</p> <p></p>"},{"location":"#hyper-v-install","title":"Hyper-V install","text":"<p>You can install Proxmox on Windows Hyper-V. Create the virtual machine with the standard procedure and execute the following command in Powershell</p> <pre><code>Set-VMProcessor -VMName &lt;Name of your Proxmox VM&gt; -ExposeVirtualizationExtensions $true\n</code></pre> <p>to switch on Hyper-V's nested virtualization. Download and mount the Proxmox ISO, after booting, click in the VM screen and tap on Enter to start the install. If your install hangs at <code>Trying to detect country</code>, disable your internet connection and restart the installer.  </p> <p>After installing Proxmox on Hyper-V, you will notice that calling an endpoint on your Hyper-V VM from WSL2 doesn't work. Open the Hyper-V Manager and next the Virtual Switch Manager</p> <p></p> <p>and note the two different virtual switches. As documented on we need to enable forwarding across both virtual switches:</p> <pre><code>Get-NetIPInterface | where {$_.InterfaceAlias -eq 'vEthernet (Default Switch)' -or $_.InterfaceAlias -eq 'vEthernet (WSL (Hyper-V firewall))'} | Set-NetIPInterface -Forwarding Enabled\n</code></pre> <p>Check setting forwarding with:</p> <pre><code>Get-NetIPInterface | select ifIndex,InterfaceAlias,AddressFamily,ConnectionState,Forwarding | Sort-Object -Property IfIndex | Format-Table\n</code></pre> <p>:fire: Note the vEthernet(...) in the IP interface aliases, if you are not sure about the aliases, first run</p> <pre><code>Get-NetIPInterface\n</code></pre> <p>:fire: This setting is disabled after rebooting!</p> <p>and check the second column.  </p> <p>Besides forwarding traffic between the (virtual) switches, you can also add a route which routes traffic destined for your Proxmox instance to the virtual switch used by your VM. In the network settings of your VM, you will have to use the IP address of your virtual switch as the gateway.</p> <p>For example, if the IP address of your VM is <code>172.18.60.44</code> and the IP address of your Hyper-V virtual switch used by your VM is <code>172.17.240.1</code>, you can route the traffic destined for your VM with:  </p> <pre><code>route -p add 172.18.60.44 MASK 255.255.255.255 172.17.240.1\n</code></pre> <p>the <code>-p</code> option ensures that the route will persist after a reboot. The <code>MASK</code> ensures that only the traffic for the VM is routed over the virtual switch. If you run several VMs, you may have to change the <code>MASK</code>.  </p> <p>Later on, you can remove this route with:</p> <pre><code>route -p delete 172.18.60.44\n</code></pre> <p>You can get the IP address of your VM by executing the following command on your VM (open a VM connection in the Hyper-V Manager):</p> <pre><code>ip addr\n</code></pre> <p>and you get the IP address of your virtual switch by executing the following command on the Windows host:  </p> <pre><code>ipconfig\n</code></pre> <p>Look for the IP address of a switch with a name like <code>Ethernet adapter vEthernet (Default Switch)</code>.  </p> <p>:fire: Note that the host (the machine running Hyper-V) may change its IP address, it is the IP address of the VM and the IP address of the switch which have to be static in this setup.  </p> <p>After adding the route, you will need to change the gateway on your VM as the route defines how to forward traffic to the VM, the path for the return traffic (the replies) will be defined by the gateway. For Proxmox edit the following file on the VM:  </p> <pre><code>nano /etc/networks/interfaces\n</code></pre> <p>and change the gateway in the interfaces config, for example:  </p> <pre><code># network interface settings; autogenerated\n# Please do NOT modify this file directly, unless you know what\n# you're doing.\n#\n# If you want to manage parts of the network configuration manually,\n# please utilize the 'source' or 'source-directory' directives to do\n# so.\n# PVE will preserve these directives, but will NOT read its network\n# configuration from sourced files, so do not attempt to move any of\n# the PVE managed interfaces into external files!\n\nauto lo\niface lo inet loopback\n\niface eth0 inet manual\n\nauto vmbr0\niface vmbr0 inet static\n        address 172.18.60.44/16\n        #gateway 172.18.60.1\n        gateway 172.17.240.1 # the ip address of your virtual switch\n        bridge-ports eth0\n        bridge-stp off\n        bridge-fd 0\n</code></pre> <p>Another option is setting up a <code>NAT</code>. As routing probably has the least impact, using routing is probably wise:  </p> <ul> <li> <p>Minimal Configuration Changes: Unlike reconfiguring network settings on either the host or the VM, routing typically involves making changes at the network infrastructure level. Once set up, these routing configurations generally require minimal maintenance and do not impact the individual devices' network settings.</p> </li> <li> <p>Supports Dynamic IP Assignment: Routing can accommodate scenarios where devices may obtain IP addresses dynamically, such as through DHCP. As long as routing configurations are properly set up, devices can communicate regardless of their assigned IP addresses.</p> </li> <li> <p>Scalability: Routing allows for scalability and flexibility. It enables communication between devices across different subnets, in our case communication between the subnet of the Windows host and the subnet of the VM.</p> </li> <li> <p>Isolation: Routing helps maintain network isolation between different segments, this is probably less relevant for this setup.</p> </li> <li> <p>Centralized Management: Routing configurations are typically managed centrally, similar to isolation, less relevant for this setup.</p> </li> </ul>"},{"location":"#bare-metal-install","title":"Bare metal install","text":"<p>Follow the default installation procedure on.</p>"},{"location":"#promox-api-user","title":"Promox API user","text":"<p>Before using the Ansible scripts, you will need a Proxmox API user:</p> <ul> <li>Log into the Proxmox Web UI</li> <li>Select datacenter from the left menu</li> <li>Select users fromf the left sub-menu</li> <li>We will use the default root@pam user</li> <li>Select API tokens from the left sub-menu</li> <li>Click add</li> <li>User: root@pam</li> <li>Token ID: input a random string, for example and record the token ID</li> <li>record the token secret</li> </ul> <p>After adding the user and API token, select Datacenter and click on Permissions and Add API Token Permission. Choose</p> <ul> <li>Path: /</li> <li>API Token: root@pam <li>Role: Administrator</li> <li>Propagate: select</li>"},{"location":"#ansible-prerequisites","title":"Ansible prerequisites","text":"<p>As we are using Ansible to automate our install and deploy, you will have to meet the following minimum requirements on the hosts which will run the Ansible scripts: Ansible minimum requirements.</p> <p>For this project, we will need some extra dependencies. Install the following Ansible role(s):</p> <pre><code>ansible-galaxy install geerlingguy.redis\n</code></pre> <p>:fire: Microsoft has an ansible SQL Server role which supports RHEL, as we use Ubuntu for now, the Microsoft Ansible SQL Server role is not usable.</p> <p>or write all dependencies to a <code>requirements.yml</code> file</p> <pre><code>---\nroles:\n  - name: geerlingguy.redis\n    type: galaxy\n    source: https://galaxy.ansible.com\n\ncollections:\n  # With just the collection name\n  - my_namespace.my_collection    \n</code></pre> <p>and install with:</p> <pre><code>ansible-galaxy role install -r requirements.yml\nansible-galaxy collection install -r requirements.yml\n</code></pre> <p>Add additional Python dependencies with:</p> <pre><code>pip install proxmoxer\n</code></pre>"},{"location":"#secrets-prerequisites","title":"Secrets prerequisites","text":"<p>Secrets are stored in env variables in files</p> <ul> <li>.env</li> <li>.env.prod</li> <li>.env.acc</li> <li>.env.dev</li> </ul> <p>Keep your PROD, ACC and DEV secrets in the corresponding files and copy to <code>.env</code> before you start the ansible scripts. <code>source</code> .env Before executing your first Ansible script.</p> <p>:fire: Obviously, none of the <code>env</code> files belongs in your repo.</p> <p>Your <code>env</code> file should at least contain your public key to connect with ssh to the VMs:</p> <pre><code>#!/bin/bash\nexport SSH_PUB_KEY=\"ssh-rsa AAAAB.... your_name@your_machine\"\n</code></pre> <p>If you are unsure about your public key, check <code>~/.ssh/id_rsa_pub</code>.</p>"},{"location":"#ansible-role-settings","title":"Ansible role settings","text":"<p>Each role has its variables (settings) defind in <code>role_folder/defaults/main.yml</code>. Variables are read from env variables:</p> <pre><code>project_name: \"{{ lookup('ansible.builtin.env', 'PROJECT_NAME') }}\"\n</code></pre> <p>You can - as usual in Ansible - easily override the default variables.  </p>"},{"location":"#gitssh-prerequisites","title":"Git/SSH prerequisites","text":"<p>When managing source code across virtual machines (VMs) on platforms like Proxmox, accessing source code from version control systems may have a security impact. A common practice involves cloning repositories using Git over SSH, authenticated via private SSH keys. However, storing private keys directly on VMs, especially those hosted on third-party servers, raises potential significant security risks. Instead, SSH agent forwarding emerges as a safer alternative.  </p> <p>SSH agent forwarding allows users to access a remote machine through SSH without placing private SSH keys on the server itself. Instead, the SSH authentication request is forwarded back to the user's local machine where the SSH agent resides. This means the private key is never exposed to the network or stored on the remote server, obviously enhancing security.</p> <p>You can setup SSH agent forwarding on your local machine with:</p> <ul> <li>Enable SSH Agent on Local Machine: Before initiating a connection, ensure the SSH agent is running on your local machine and your private key is added. This can be done using:</li> </ul> <pre><code>eval $(ssh-agent -s)  \n# note the full path, if you are using WSL and the windows agent (1password, etc.) you have to be explicit about \"which ssh config\" you are changing (Ansible does not use the 1password Windows agent)\n/usr/bin/ssh-add # add all keys under ~/.ssh/\n/usr/bin/ssh-add -l # list all keys\n</code></pre> <ul> <li>Configure SSH to Use Agent Forwarding: Modify the SSH configuration file (~/.ssh/config) on your local machine to enable agent forwarding. You can specify this globally or for specific hosts:</li> </ul> <pre><code>Host 192.168.1.*    \n  ForwardAgent yes\n</code></pre> <ul> <li>Connect to the VM: Once configured, you can connect to the VM using SSH. The agent forwarding will carry your authentication from the local machine to the VM:</li> </ul> <pre><code>/usr/bin/ssh-add # note the full path, if you are using WSL and the windows agent (1password, etc.) you have to be explicit about \"which ssh config\" you are changing (Ansible does not use the 1password Windows agent)\n/usr/bin/ssh -v -T git@github.com\n#Hi your_account! You've successfully authenticated, but GitHub does not provide shell access.\n/usr/bin/ssh root@192.168.1.60 # SSH into your host, you can test this once your hosts are \"online and reachable\"\necho $SSH_AUTH_SOCK #response should not be empty\n#/tmp/ssh-n9uZGGjGAC/agent.2806\nssh -v -T git@github.com # on your host, you probably are not using WSL and the windows agent  \n#Hi username You've successfully authenticated, but GitHub does not provide shell access.\n</code></pre> <ul> <li>Clone Repository Using SSH on the VM: On the VM, use Git to clone the repository as usual. The SSH agent will authenticate using your local machine\u2019s private key, as an example (will be executed in Ansible scripts):  </li> </ul> <pre><code>git clone git@github.com:user/repository.git\n</code></pre> <p>Some advantages of SSH agent forwarding:</p> <ul> <li>Enhanced Security: The primary advantage is security. Your private SSH keys remain secure on your local machine, reducing the risk of exposure through VM compromise.</li> <li>Ease of Use: Once set up, the process is transparent to the user. You can seamlessly authenticate to Git repositories without managing multiple keys on different VMs.</li> </ul> <p>Some drawbacks and potential considerations:</p> <ul> <li>Security Risks with Misconfiguration: If not configured properly, SSH agent forwarding can expose your local SSH agent to remote machines, which could be a vector for an attack if the remote machine is compromised.</li> <li>Dependency on Local Machine: Your local machine must be online and accessible whenever operations requiring SSH authentication are performed, potentially limiting flexibility.</li> <li>Complex Setup: The initial setup requires understanding of SSH configurations and might be more complex compared to simply copying keys to servers.</li> </ul> <p>Some best practices:</p> <ul> <li>Limit Forwarding: Only enable agent forwarding to trusted hosts to mitigate the risk of exposing your SSH agent to a compromised server.</li> <li>Monitor Sessions: Keep an eye on active SSH sessions that utilize agent forwarding to ensure they are legitimate and necessary.</li> <li>Security Hardening: Regularly update and patch both local and remote systems to protect against vulnerabilities.</li> </ul> <p>:fire: If your source code is hosted on a public repository, you can off course skip the steps above.  </p>"},{"location":"#proxmox-firewall-preparation","title":"Proxmox Firewall Preparation","text":"<p>Before you can install a firewall, you have to make sure that at least one of your VMs an access the Internet, all other VMs on Proxmox have to be able to communicate with each other in a LAN only. No Internet connectivity is required for the \"non firewall\" VMs.  </p> <p>In order to arrange this setup, we will need two bridges:</p> <ul> <li>one bridge which will bridge the incoming Internet traffic to the firewall and vice-versa</li> <li>one bridge which will allow the VMs to communicate within a LAN</li> <li>the VMs within the LAN use the LAN IP address of the firewall as gateway to return traffic over the firewall/VPN</li> </ul> <p>A step-by-step guide to achieve this:</p> <ul> <li>You should have one existing bridge with a IPv4 which equals your public IP address (not the additional IP address)</li> <li>edit the MAC address of the network interface of the VM which has to access the Internet, enter the MAC address of the additional IP address</li> <li>Create a New Bridge for the Internal Network</li> <li>Log into Proxmox VE Web Interface: Open your browser and access the Proxmox VE web interface by navigating to your Proxmox server's IP address.  </li> <li>Navigate to 'System' -&gt; 'Network': Here you will see your current network configuration, including any bridges and physical interfaces.</li> <li>Create a New Bridge:<ul> <li>Click on \"Create\" and select \"Linux Bridge\".</li> <li>Give the bridge a meaningful name, like vmbr1 (assuming vmbr0 is your existing bridge for the internet connection).</li> <li>Assign it an IP address that fits within your internal network scheme (e.g., 192.168.1.1/16). If the Proxmox host doesn't need to communicate on this network, you may leave the IP address field empty.</li> <li>Leave the Gateway field empty (the existing bridge has a Gateway)</li> <li>Ensure the \"Autostart\" option is checked.</li> <li>Leave the \"Bridge ports\" field empty if this bridge is for internal VM communication only.</li> <li>Click \"Create\".</li> </ul> </li> <li>Attach a Second Network Interface to Your VM</li> <li>Select Your VM: In the Proxmox web interface, go to the \"Server View\", find the VM you want to configure, and click on it.</li> <li>Add Network Device:<ul> <li>With the VM selected, go to the \"Hardware\" tab.</li> <li>Click \"Add\" and choose \"Network Device\".</li> <li>For \"Model\", you can choose \"VirtIO (paravirtualized)\" for better performance or \"e1000\" for broader compatibility.</li> <li>Under \"Bridge\", select the new bridge you created (vmbr1).</li> <li>Click \"Add\".</li> </ul> </li> <li>Configure the Operating System Inside the VM: After attaching the second network interface, you need to configure the operating system inside the VM to use this new interface.  </li> <li>Access Your VM: Log into your VM via the Proxmox console or SSH.</li> <li>Identify the New Interface: Run ip a or ifconfig to list your network interfaces. You should see a new one (likely named eth1 or similar).</li> <li>Configure the Network Interface: You'll need to edit the network configuration file or use a network manager to configure the interface. The exact steps vary depending on your Linux distribution, for Ubuntu:</li> </ul> <pre><code># /etc/network/interfaces - create the file if it does not exist\nauto eth1\niface eth1 inet static\n    address 192.168.1.x\n    netmask 255.255.0.0\n    gateway 192.168.1.1\n</code></pre> <p>For pfSense, you don't need to edit /etc/network/interfaces or /etc/netplan/..., it is sufficient to add the second network interface and assign its IP address in the pfSense UI. * Restart the Network Service or the VM to apply changes.  </p>"},{"location":"#bridges","title":"Bridges","text":"<p>The terms bridge and switch are often used interchangeably in the context of networking, but they refer to devices that operate at the data link layer (Layer 2) of the OSI model.  </p> <p>A bridge in Proxmox (or any other virtualization platform) is conceptually closer to a physical switch than to a traditional physical bridge in terms of its functionality and use case. Here's why:</p> <ul> <li> <p>Virtual Bridges and Physical Switches: Similarities</p> </li> <li> <p>Multiple Connections: Both virtual bridges in Proxmox and physical switches are designed to connect multiple devices (or virtual machines/containers in the case of Proxmox) within a network. They allow for the creation of network segments that can communicate internally and externally.</p> </li> <li>Traffic Management: Like a physical switch, a virtual bridge can manage and forward traffic between its connected interfaces based on MAC addresses, efficiently directing packets to their intended destinations within the virtual network or to external networks. </li> <li> <p>Advanced Networking Features: While not as feature-rich as some high-end physical switches, virtual bridges in Proxmox can still offer several advanced networking features, such as VLAN tagging, which is a hallmark of switch capabilities.</p> </li> <li> <p>Differences from Physical Bridges</p> </li> <li> <p>Functionality: Traditional physical bridges were primarily used to connect two network segments, with a focus on reducing collision domains and managing traffic between these segments. While a Proxmox bridge can perform a similar role in a virtual environment, it more closely mirrors the multiport, multipurpose functionality of switches by allowing numerous virtual machines and containers to connect to various networks.</p> </li> <li> <p>Port Density: Physical bridges typically have a very limited number of ports (often just two), akin to the simplest form of segmentation. In contrast, a virtual bridge in Proxmox can handle connections from numerous VMs and containers simultaneously, much like a physical switch with many ports.</p> </li> <li> <p>Use in Virtualization</p> </li> <li> <p>Network Virtualization: Virtual bridges play a crucial role in network virtualization, providing a platform for VMs and containers to communicate as if they were connected to a physical switch. This is essential for creating complex virtual network topologies that resemble physical network infrastructures.</p> </li> <li>Flexibility and Scalability: In virtual environments, bridges offer a level of flexibility and scalability that is more characteristic of switches. Administrators can dynamically adjust network configurations, add or remove VMs from networks, and implement security policies or VLANs without needing physical hardware changes.</li> </ul> <p>In summary, while the terminology may suggest a direct analogy to physical bridges, in practice, the role of a bridge in Proxmox and other virtualized environments aligns more closely with the functionalities of a physical switch, especially concerning its ability to connect multiple devices and manage network traffic efficiently within a virtualized networking context.</p> <p>For our setup, that becomes:</p> <pre><code>graph TB\n  subgraph Internet\n  end\n  subgraph Proxmox\n    direction TB\n\n    subgraph vmbr0 [vmbr0&lt;br&gt;Public IPv4: 37.187.28.32&lt;br&gt;Gateway: 37.187.28.254]\n\n    end\n\n    subgraph VM500 [VM 500 - PfSense]\n      direction TB\n      eth0(eth0&lt;br&gt;MAC Additional IP)\n      eth1(eth1&lt;br&gt;IP: 192.168.1.51)\n      eth0 &lt;--firewall--&gt; eth1\n    end\n\n    subgraph vmbr1 [vmbr1&lt;br&gt;LAN Subnet: 192.168.1.1/16&lt;br&gt;Gateway: 192.168.1.51]\n\n    end       \n\n    subgraph LAN [LAN Subnet: 192.168.1.1/16&lt;br&gt;Gateway: 192.168.1.51]\n\n    end\n\n    vmbr0 --&gt; eth0\n    eth1 --&gt; vmbr1\n    vmbr1 --&gt; LAN    \n  end \n  Internet &lt;---&gt; vmbr0  </code></pre>"},{"location":"#proxmox","title":"Proxmox","text":""},{"location":"#proxmox-vm-setup","title":"Proxmox VM setup","text":"<p>As far as I know, Ansible is not necessarily the tool of choice to setup VMs on cloud/datacenter solutions like Proxmox, etc. Nevertheless, as an excercise, I've chosen to setup my Proxmox VMs with Ansible.  </p> <p>Ansible Uses an <code>inventory</code> to define the machines (ip addresses) and machine roles to install/setup your software infrastructure. As we start with a clean install (no existing VMs), it is rather difficult to define an <code>inventory</code>.  </p> <p>Therefore, we start with a play proxmox which uses role and vars file defining the inventory, for example:</p> <pre><code>proxmox_vms:\n - ip: 192.168.1.60\n   id: 600\n   group:\n    - \"webserver\"\n - ip: 192.168.1.61\n   id: 601\n   group:\n    - \"redis\"\n - ip: 192.168.1.62\n   id: 602\n   group:\n    - \"database\"\n</code></pre> <p>On the other hand, it is probably wise to structure/group ip addresses and vm ids under <code>group:</code> which is more aligned with an Ansible inventory (and easier to maintain). :fire: That's for another version.  </p> <p>After running the proxmox play, we can use the initialized VMs as defined in the vars file as the basis for the Ansible <code>\u00ecnventory</code>.  </p> <p>Running the proxmox play is detailed in Uploading templates and Installing VMs on Proxmox.  </p> <p>:fire: The hosts file contains the <code>inventory</code> for the other Ansible plays. :fire: At this moment, the hosts file is not generated from the proxmox play, it is probably a good idea to change that in future versions.  </p>"},{"location":"#uploading-proxmox-vms-templates","title":"Uploading Proxmox VM(s) templates","text":"<pre><code>ansible-playbook proxmox.yml -K --tags=vm_upload -vvv\n</code></pre> <p>:fire: If this tag fails, you may have to upgrade your community.general modules, see issue:</p> <pre><code>ansible-galaxy collection install community.general\n</code></pre> <p>you should have at least version 7.2.1</p> <pre><code>ansible-galaxy collection list\n</code></pre> <p>and look for community.general.</p> <p>:fire: You probably have two installs of community.general, if you'd like to run the system-wide install:</p> <pre><code>sudo ansible-galaxy collection install community.general\n</code></pre>"},{"location":"#installing-proxmox-vms","title":"Installing Proxmox VM(s)","text":"<pre><code>ansible-playbook proxmox.yml -K --tags=vm_init -vvv\n</code></pre>"},{"location":"#addding-proxmox-vms-to-known_hosts","title":"Addding Proxmox VM(s) to known_hosts","text":"<p>If you removed existing VMs/Containers or are building new VMs/Containers, you should remove the old entries in <code>known_hosts</code> and add new entries:</p> <pre><code>ANSIBLE_HOST_KEY_CHECKING=false ansible-playbook knowhosts-setup.yml -i hosts -K -vvv\n</code></pre> <p>When you observe an additional line being added to your known_hosts file after connecting to a server via SSH, even after manually adding the server's fingerprint with ssh-keyscan or with <code>ansible-playbook knowhosts-setup.yml</code>, it typically relates to how SSH handles and verifies the identity of the servers it connects to. There are a few reasons why this might happen:</p> <ol> <li> <p>Different Key Types If you initially add the server's <code>ED25519</code> key fingerprint to your known_hosts using ssh-keyscan -H -t ed25519 ip_address, but the server is also configured to use another type of SSH key (e.g., RSA, ECDSA), the SSH client might add the fingerprint of this additional key to the known_hosts file upon the first connection. This occurs because your initial scan and add operation only included the ED25519 key, and upon connection, SSH automatically adds any other keys presented by the server that weren't already in known_hosts.</p> </li> <li> <p>Hostname and IP Address Entries Another common reason is the difference in how you reference the server in your ssh command versus what was initially scanned. For instance, if you scanned the IP address and then used the hostname (or vice versa) to connect, SSH treats these as separate entries. SSH distinguishes between IP addresses and hostnames because they can technically present different keys (consider virtual hosts or shared IP scenarios). As a result, SSH might add a new line for the same server under its different identifier (IP or hostname).</p> </li> <li> <p>SSH Configuration and Aliases Your SSH client's configuration might influence how known_hosts is managed. For example, if you use an SSH config file (~/.ssh/config) with aliases or specific host entries that define hostname patterns or specific key types, your SSH client might treat connections that match different patterns as distinct, even if they ultimately resolve to the same server.</p> </li> <li> <p>Port Forwarding or Jump Hosts If your connection involves port forwarding or using a jump host (also known as a bastion host), the SSH client may add entries for these intermediate steps. This is more likely in complex networking setups where direct connections to the target server are not possible without going through intermediary servers.</p> </li> </ol> <p>Troubleshooting Tips:</p> <ul> <li>Review the known_hosts file: Compare the entries before and after the connection. Look for differences in the key types, hostnames/IP addresses, or additional details that might explain the new entry.</li> <li>Use verbose mode with SSH: Connecting with <code>ssh -v root@ip_address</code> can provide detailed logs that explain what keys are being checked, offered, and added to known_hosts. This might give you a clearer picture of why the additional line is added.</li> <li>Check SSH client configuration: Review your SSH client's configuration file (if you have one) for any settings that might affect how known_hosts entries are managed or how connections are established.</li> </ul> <p>Understanding the exact reason requires examining the specifics of your SSH setup, the server configuration, and the entries in known_hosts.</p> <p>For our particular setup, we can edit the server config <code>sshd_config</code> file on the server we connect to to enforce the usage of <code>ED25519</code> keys:</p> <pre><code>#HostKey /etc/ssh/ssh_host_rsa_key\n#HostKey /etc/ssh/ssh_host_ecdsa_key\nHostKey /etc/ssh/ssh_host_ed25519_key\nKexAlgorithms curve25519-sha256,curve25519-sha256@libssh.org\nPubkeyAcceptedKeyTypes ssh-ed25519\n</code></pre> <p>in addition, make sure that folder <code>/run/sshd</code> exists on the server:</p> <pre><code>mkdir /run/sshd\nchmod 0755 /run/sshd\nsystemctl restart sshd\n</code></pre> <p>Clients, however, should now support <code>ED25519</code>, i.e. use:</p> <pre><code>ssh-keygen -t ed25519 -C you@cars.be\n</code></pre> <p>If you now connect with:</p> <pre><code>ssh -v root@192.168.1.68\n</code></pre> <p>you will notice in the output that ssh will attempt different keys, for example:</p> <pre><code>debug1: Will attempt key: /home/you/.ssh/id_rsa RSA SHA256:....\ndebug1: Will attempt key: /home/you/.ssh/id_ecdsa\ndebug1: Will attempt key: /home/you/.ssh/id_ecdsa_sk\ndebug1: Will attempt key: /home/you/.ssh/id_ed25519 ED25519 SHA256:....\ndebug1: Will attempt key: /home/you/.ssh/id_ed25519_sk\ndebug1: Will attempt key: /home/you/.ssh/id_xmss\ndebug1: Will attempt key: /home/you/.ssh/id_dsa\n</code></pre> <p>To conclude, the server decides about the security of the keys used by the client, therefore, we will configure all sshd daemons for <code>ED25519</code>:  </p> <pre><code>ansible-playbook sshdserver_setup.yml -i hosts --user root -K -vvv\n</code></pre> <p>:fire: User root is specified with <code>--user root</code>: the hosts we connect to have the <code>ED25519</code> key under <code>/root/.ssh/authorized_keys</code>, so we have to connect as <code>root</code> to use this key.  </p>"},{"location":"#stopping-proxmox-vms","title":"Stopping Proxmox VM(s)","text":"<pre><code>ansible-playbook proxmox.yml -K --tags=vm_stop -vvv\n</code></pre>"},{"location":"#starting-proxmox-vms","title":"Starting Proxmox VM(s)","text":"<pre><code>ansible-playbook proxmox.yml -K --tags=vm_start -vvv\n</code></pre> <p>if necessary, you can restart with</p> <pre><code>ansible-playbook proxmox.yml -K --tags=vm_restart -vvv\n</code></pre>"},{"location":"#removing-proxmox-vms","title":"Removing Proxmox VM(s)","text":"<p>Before removing a Proxmox VM, first stop the Proxmox VM, so execute the stopping proxmox command or add tag vm_stop before the vm_remove tag in the following command:</p> <pre><code>ansible-playbook proxmox.yml -K --tags=vm_stop,vm_remove -vvv\n</code></pre>"},{"location":"#redis","title":"REDIS","text":"<p>Cross-Site Request Forgery (CSRF) is a security threat where an attacker tricks a user into executing unwanted actions on a web application in which they're authenticated. If the victim is a regular user, a successful CSRF attack can force them to perform state-changing requests like transferring funds, changing their email address, and so forth. If the victim has an administrative account, CSRF can compromise the entire web application. CSRF exploits the trust that a site has in the user's browser, and unlike Cross-Site Scripting (XSS), which exploits the trust a user has in a particular site, CSRF exploits the trust that a site has in the user's browser.</p> <p>In a distributed web application architecture, particularly one that scales horizontally as in this setup, requests from the same user can be routed to different servers across multiple requests due to load balancing. This poses a challenge for CSRF protection mechanisms that rely on keeping track of state, such as synchronizer tokens or double submit cookies, because the server handling a subsequent request might not have access to the tokens generated by another server on a previous request.</p> <p>REDIS, as a fast, in-memory data store offers a solution for storing CSRF protection keys in such a distributed setup. </p> <p>In the drawing above, REDIS was added explicitely to stress the impact of the distributed nature and horizontal scalability of the setup, i.e. its non-trivial impact on CSRF. More details are available on the cars.be repo.</p>"},{"location":"#install-redis","title":"Install REDIS","text":"<pre><code>ansible-playbook redis.yml -i hosts -K -vvv\n</code></pre> <p>:fire: Note the <code>hosts</code> file, at the moment, only ip address are used in the host file. In an ideal case, hostnames as setup on Proxmox should be used grouped by labels redis, webserver, etc.</p> <p>:fire: If you get an error like</p> <pre><code> \"msg\": \"Unable to start service redis-server: Job for redis-server.service failed because the control process exited with error code.\\nSee \\\"systemctl status redis-server.service\\\" and \\\"journalctl -xe\\\" for details.\\n\"\n</code></pre> <p>after running the redis playbook, you probably forgot to <code>source .env</code>.  </p>"},{"location":"#authentication-server","title":"Authentication Server","text":""},{"location":"#install-authentication-server","title":"Install Authentication Server","text":"<p><code>ABP</code> supports IdentityServer4 and OpenIddict. As the <code>ABP</code> startup templates support OpenIddict since ABP v6.0.0 we will only support <code>OpenIddict</code>.  </p> <pre><code>ansible-playbook authentication.yml -i hosts -K -vvv\n</code></pre> <p>After installing the authentication server, you can request a client credentials (server to server), check the documentation and example in the cars.be repo.</p>"},{"location":"#rest-api","title":"REST API","text":""},{"location":"#install-rest-api","title":"Install REST API","text":"<pre><code>ansible-playbook abprestapi.yml -i hosts -K -vvv\n</code></pre> <p>The REST API contains a custom metric and controller, open the REST API interface (<code>Swagger</code>) and call <code>/api/car/increment</code> to increment the custom metric. If you'd like to monitor this metric in real-time, SSH into the HTTP server and run:  </p> <pre><code>#if nessary, install dotnet-counters\n#dotnet tool install --global dotnet-counters\ndotnet-counters monitor --name Be.Cars.HttpApi.Host --counters Be.Cars.Metrics.CustomMetrics\n</code></pre>"},{"location":"#proxy","title":"Proxy","text":""},{"location":"#install-nginx-proxy","title":"Install NGINX Proxy","text":"<pre><code>ansible-playbook nginx.yml -i hosts -K -vvv\n</code></pre>"},{"location":"#database","title":"Database","text":""},{"location":"#install-sql-server","title":"Install SQL Server","text":"<pre><code>ansible-playbook mssql.yml -i hosts -K -vvv\n</code></pre> <p>After installing your DB server, you can SSH into the machine and open an SQL shell with (assuming you use password <code>p@55w0rD</code>):</p> <pre><code>sqlcmd -S 127.0.0.1 -U sa -P p@55w0rD -C\n</code></pre>"},{"location":"#web-server","title":"Web server","text":""},{"location":"#install-the-blazor-application","title":"Install the Blazor Application","text":"<p>For this project, we will host the application (Blazor application) on Kestrel. <code>Kestrel</code> is automatically included by publishing the application.</p> <pre><code>ansible-playbook webserver.yml -i hosts -K -vvv\n</code></pre> <p>:fire: Install the web server after installing the database: while installing the web server, the DB migration project will run and initialize the database.  </p>"},{"location":"#application-performance-monitoring","title":"Application Performance Monitoring","text":"<p>In this project we will use Prometheus as it supports OTLP (Open Telemetry Protocol) which is used in the example code (web site and REST API). You can also export OTel (Open Telemetry) data to Jaeger or Zipkin. For an overview, please check the following list of  APM (Application Performance Monitoring) vendors.</p> <pre><code>flowchart TD;    \n    WebServer[Web Server]-- \":5000/metrics\" --&gt;Prometheus;\n    RESTAPI[REST API]-- \":5000/metrics\" --&gt;Prometheus;\n    Prometheus-- \":9090\" --&gt;Grafana</code></pre> <p>:fire: The example code is written in dotnet core, which has its own instrumentation APIs for logging, metrics and tracing. Therefore, OTel collects telemetry from the build in dotnet core APIs. The benefit that OTel brings as an industry standard is a common mechanism for collecting telemetry data and integrating APMs. For more information see.</p> <p>:fire: If you are using a different programming language, please check the supported programming languages.  </p>"},{"location":"#prometheus-server","title":"Prometheus server","text":"<p>As illustrated in the figure above, Prometheus is configured with the metrics endpoints of the Web Server and REST API. Extend this configuration if you'd like to inlude other systems/microservices/etc.</p>"},{"location":"#install-prometheus-server","title":"Install Prometheus server","text":"<pre><code>ansible-playbook prometheus.yml -i hosts -K -vvv\n</code></pre>"},{"location":"#collector","title":"Collector","text":"<p>For a more flexible setup, you may be interested in the setup of an Otel collector. For example <code>InfluxDB</code> with Telegraf and the OpenTelemetry Input Plugin:</p> <pre><code>flowchart TD;    \n    WebServer[Web Server]-- \"OTLP GRPC\" --&gt;Telegraf;\n    RESTAPI[REST API]-- \"OTLP GRPC\" --&gt;Telegraf;\n    Telegraf----&gt;InfluxDB;\n    Telegraf----&gt;Kapacitor;\n    Kapacitor&lt;----&gt;Chronograf\n    Kapacitor&lt;----&gt;InfluxDB;\n    InfluxDB----&gt;Grafana;\n    InfluxDB----&gt;Zipkin/Prometheus/...;\n    InfluxDB&lt;----&gt;Chronograf;</code></pre> <p>For completeness, the diagram above mentions Chronograf the user interface and administrative comonent of InfluxDB which allows to interact with the TICK-stack and Zipkin a distributed tracing system.  </p> <p>You can also use the OpenTelemetry Collector.  </p>"},{"location":"#install-influxdb","title":"Install InfluxDB","text":"<pre><code>ansible-playbook otelcollector.yml -i hosts -K -vvv\n</code></pre> <p>After installing, open the UI on <code>http://{{ groups['otlp_controller'][0] }}:8086)</code>.</p>"},{"location":"#grafana-server","title":"Grafana server","text":""},{"location":"#install-grafana-server","title":"Install Grafana server","text":"<pre><code>ansible-playbook grafana.yml -i hosts -K -vvv\n</code></pre> <p>After installing Grafana, you can browse to the Grafana endpoint (check the Grafana endpoint in hosts) and open the <code>ASP .NET Core</code> dashboard: </p> <p></p>"},{"location":"#certificate-authority","title":"Certificate Authority","text":"<p>Setting up a CA chain with Ansible and automating the process of generating CSRs (Certificate Signing Requests) for each virtual machine/container is a robust and secure way to manage certificates in our infrastructure. For developers, it is probably sufficient to generate certificates used to encrypt internal traffic and for your public endpoint in your LAN. If you'd like to publish your public endpoint on the Internet, you should use a CA like Let's Encrypt.  </p> <ol> <li> <p>Ansible Roles:</p> <ul> <li>CA Role: This role sets up a Certificate Authority (CA).</li> <li>Client Role: This role will generate CSRs for each virtual machine and handle the signing of certificates by the CA.</li> </ul> </li> <li> <p>CA Role:</p> <ul> <li>Configure the CA server: Install and configure software like OpenSSL to act as the CA.</li> <li>Generate the CA certificate and private key.</li> <li>Create necessary directories to store certificates and keys.</li> <li>Set up appropriate permissions to ensure security.</li> </ul> </li> <li> <p>Client Role:</p> <ul> <li>Install necessary tools like OpenSSL for generating CSRs.</li> <li>Generate CSRs for each virtual machine.</li> <li>Securely transmit the CSRs to the CA server.</li> <li>Sign the CSRs.</li> <li>Retrieve signed certificates from the CA.</li> </ul> </li> </ol> <p>By following the steps above, we can set up our own (small/simple) CA chain with Ansible and automate the process of generating CSRs and signing certificates for our virtual machines/internal traffic.</p> <p>To ensure that consumers trust our client certificates and/or to establish secure communication channels within our infrastructure (network communication within the datacentre), we have several options:</p> <ul> <li>Distribute CA Certificate: Distribute the CA certificate (which was used to sign the client certificates) to the consumers of the client certificates. Consumers can then import the CA certificate into their trust store (e.g., browser's certificate store, operating system's certificate store) as a trusted root CA. This allows the consumers to trust any certificates signed by that CA. :cupid: If you run the Ansible playbooks on Ubuntu, the <code>caclient</code> playbook will install the root certificate for you, on Windows, you will have to install the certificate manually (at least, for now).</li> <li>Certificate Chain: Along with the client certificate, provide the entire certificate chain up to the root CA certificate. This includes the client certificate, any intermediate CA certificates, and the root CA certificate. Consumers can then verify the certificate chain, ensuring that each certificate in the chain is signed by the preceding one, up to the trusted root CA certificate. :fire: In this particular implementation, we have no certificate chain.</li> <li>Publicly Trusted CA: If you require wider trust, consider obtaining client certificates from a publicly trusted CA (e.g., Let's Encrypt, DigiCert). Certificates issued by publicly trusted CAs are automatically trusted by most systems, as they are included in the trust stores of popular browsers and operating systems. :fire: For our public endpoint, we will use Let's Encrypt, internal traffic, however, will be encrypted with the self-generated client certificates.</li> <li>Custom Trust Store: Create a custom trust store containing the CA certificate or certificate chain and distribute it to the consumers of the client certificates. Applications can then be configured to use this custom trust store to verify the authenticity of client certificates. :fire: Might be used in a future implementation.</li> <li>Certificate Pinning: Implement certificate pinning, where consumers explicitly trust a specific client certificate or CA certificate by embedding its fingerprint or public key within the application code. This ensures that only the specified certificate or CA is trusted, providing an additional layer of security. :fire: Might be used in a future implementation.</li> <li>Education and Documentation: Educate consumers about the trust model and security practices involved in certificate authentication. Provide clear documentation on how to verify and trust client certificates, including instructions for importing CA certificates or certificate chains into trust stores. :fire: Might be used in a future implementation.</li> </ul> <p>In our implementation, the Client Role will copy the CA certificate to the default Ubuntu certificate stores.</p>"},{"location":"#install-certificate-authority","title":"Install Certificate Authority","text":"<pre><code>ansible-playbook ca.yml -i hosts -K -vvv\n</code></pre>"},{"location":"#generate-client-certificates","title":"Generate Client Certificates","text":"<pre><code>ansible-playbook caclient.yml -i hosts -K -vvv\n</code></pre>"},{"location":"#https-offloading","title":"HTTPS Offloading","text":"<p>In the previous section, we explained how certificates are generated with our own CA. We can now use the certificates to encrypt internal traffic after offloading HTTPS traffic.</p> <p>As we'd like to use HTTPS offloading and load balance the web server and REST endpoint, we migth choose a setup similar to:</p> <pre><code>graph TB\n    client1(Internet Client 1) --&gt;|HTTPS request| nginx[Load Balancing&lt;br/&gt;HTTPS offloading]\n    client2(Internet Client 2) --&gt;|HTTPS request| nginx\n    client3(Internet Client 3) --&gt;|HTTPS request| nginx\n\n    nginx --&gt;|Distribute request| ws1[Web Server 1]\n    nginx --&gt;|Distribute request| ws2[Web Server 2]\n    nginx --&gt;|Distribute request| re1[REST Endpoint 1]\n    nginx --&gt;|Distribute request| re2[REST Endpoint 2]\n\n    ws1 --&gt;|Optional state management| redis[(REDIS Instance)]\n    ws2 --&gt;|Optional state management| redis\n    re1 --&gt;|Optional state management| redis\n    re2 --&gt;|Optional state management| redis</code></pre> <p>Note that internal traffic (for example the Web Server querying the REST endpoint) runs over a publically acessible proxy and is not necessarily encrypted. Moreover, if the proxy acting as the HTTPS offloader and load balancer is compromised, an attacker could potentially gain access to both the unencrypted internal traffic routed through it and the encrypted traffic it decrypts, which makes it a critical point of security.  </p> <p>Implementing a Demilitarized Zone (DMZ) is an approach to solve this: mitigate risks by segregating different parts of a network, hence, limiting the potential impact of a breach. A potential approach:  </p> <pre><code>graph TD\n    subgraph Internet\n      client[Client]\n    end\n\n    subgraph DMZ\n      nginx_public[ProxyPublic]\n    end\n\n    subgraph Internal Network\n      nginx_internal[ProxyInternal]\n      rest[REST Endpoints]\n      web[Web Servers]\n      redis[(REDIS)]\n    end\n\n    client --&gt; nginx_public\n    nginx_public -.-&gt;|Secure Channel| nginx_internal\n    nginx_internal --&gt; rest\n    nginx_internal --&gt; web\n    rest --&gt;|State Management| redis\n    web --&gt;|State Management| redis</code></pre> <p>From the diagram above, the segregation of both networks should be obvious: internal traffic is routed over <code>ProxyInternal</code> and not visible on (a breached) <code>ProxyPublic</code>.  </p> <p>As the proxy acts as the entry point for incoming traffic, it can offload SSL/TLS for incoming HTTPS requests. The proxy decrypts the HTTPS traffic, re-encrypts the traffic and then routes the requests to the appropriate backend services. Backend services can - for instance - use certificates from an internal Certificate Authority (CA).  </p> <pre><code>graph TD\n    subgraph Internet\n      client[Client]\n    end\n\n    subgraph DMZ\n      nginx_public[ProxyPublic]\n    end\n\n    subgraph Internal Network\n      nginx_internal[ProxyInternal]\n      rest[REST Endpoints]\n      web[Web Servers]\n      redis[(REDIS)]\n    end\n\n    client --&gt;|HTTPS| nginx_public    \n    nginx_public --&gt;|HTTPS| nginx_internal\n    nginx_internal --&gt;|HTTPS| rest\n    nginx_internal --&gt;|HTTPS| web\n    rest --&gt;|State Management| redis\n    web --&gt;|State Management| redis        </code></pre> <p>:fire: TODO to configure a DMZ (firewall), we need some extra info: | Source IP | Destination IP | Protocol (HTTPS TCP/443 or DNS UDP/53 or ...) | |:---------:|:--------------:|:-----------------------------------------:| | 192.168.51|192.168.1.60    |HTTPS TCP/443                              |</p>"},{"location":"#nginx","title":"NGINX","text":"<p>The proxy above may be implemented with different solutions:</p> <ul> <li>NGINX</li> <li>Traefik</li> <li>HAProxy</li> <li>Apache HTTP Server</li> <li>IIS</li> <li>Envoy Proxy</li> <li>Caddy</li> <li>F5</li> <li>...</li> </ul> <p>For this paricular setup, we will use NGINX.  </p> <p>The following NGINX configuration demonstrates a setup designed to facilitate secure HTTPS connections to a load-balanced backend comprised of multiple HTTPS API servers:</p> <pre><code>flowchart TD;        \n    RESTAPI1[REST Endpoint node1];\n    RESTAPI2[REST Endpoint node2];\n    RESTAPI3[REST Endpoint node2];\n\n    NGINX--load balances/HTTPS--&gt;RESTAPI1;\n    NGINX--load balances/HTTPS--&gt;RESTAPI2;\n    NGINX--load balances/HTTPS--&gt;RESTAPI3;\n\n    RESTAPI1--data protection--&gt;Redis;\n    RESTAPI2--data protection--&gt;Redis;\n    RESTAPI3--data protection--&gt;Redis;\n\n    Client--HTTPS--&gt;NGINX</code></pre> <p>The <code>NGINX</code> configuration for the following setup illustrates a setup for managing encrypted, load-balanced connections to a backend API cluster, with a focus on detailed logging, security with SSL, and flexibility to support large headers and optional WebSocket connections:  </p> <pre><code>http {\n    log_format upstreamlog '[$time_local] $remote_addr - $remote_user - $server_name $host to: $upstream_addr: $request $status upstream_response_time $upstream_response_time msec $msec request_time $request_time';\n\n    upstream https_api {\n        server 192.168.1.64:5000;\n        server 192.168.1.68:5000;\n        server 192.168.1.69:5000;\n    }\n\n    server {\n        listen 443 ssl;\n        listen [::]:443 ssl;\n        include snippets/self-signed.conf;\n        include snippets/ssl-params.conf;\n\n        client_header_buffer_size 4k;\n        large_client_header_buffers 4 8k; # Increase if JWT or other headers are large      \n\n        access_log /var/log/nginx/access.log upstreamlog;\n\n        # The location ^~ /api/ block captures all requests starting with /api/ and forwards them to the https_api upstream. The ^~ modifier gives this location precedence over regular expression locations that might also match.\n        location ^~ /api/ {\n            # The proxy_pass https://https_api; directive tells NGINX to forward the requests to the defined upstream group https_api. Ensure the scheme (https://) matches your upstream configuration. If your upstream servers are configured with HTTP, change the scheme accordingly.\n            proxy_pass https://https_api;\n\n            # Header adjustments are defined inside the /api/ location block to ensure they're applied only to the proxied requests.          \n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_set_header Authorization $http_authorization;\n\n            # Additional necessary proxy settings\n        }\n\n        # The location / block is modified to return a 404 error for any requests that don't match the /api/ path. This effectively drops traffic not intended for the API.        \n        location / {\n            return 404;\n        }\n\n      # Optional: Redirect the root path specifically if necessary\n      # location = / {\n      #     return 404;\n      # }\n\n      # Other possible headers you might want to set\n      # proxy_set_header Authorization \"\"; # If you need to reset the Authorization header\n\n      # WebSocket support (if needed)\n      # proxy_http_version 1.1;\n      # proxy_set_header Upgrade $http_upgrade;\n      # proxy_set_header Connection \"upgrade\";\n    }\n}\n</code></pre> <p>If we'd like to include the web servers in the diagram and configured as above, we get a diagram like:</p> <pre><code>flowchart TD;        \n    subgraph /\n        web1[Web Server node1]\n        web2[Web Server node2]    \n    end\n    subgraph /api\n        RESTAPI1[REST Endpoint node1];\n        RESTAPI2[REST Endpoint node2];\n        RESTAPI3[REST Endpoint node2];\n    end\n\n    NGINX--load balances/HTTPS--&gt;RESTAPI1;\n    NGINX--load balances/HTTPS--&gt;RESTAPI2;\n    NGINX--load balances/HTTPS--&gt;RESTAPI3;\n\n\n    NGINX--load balances/HTTPS--&gt;web1;\n    NGINX--load balances/HTTPS--&gt;web2;    \n\n\n    web1--data protection--&gt;Redis;\n    web2--data protection--&gt;Redis;   \n\n    RESTAPI1--data protection--&gt;Redis;\n    RESTAPI2--data protection--&gt;Redis;\n    RESTAPI3--data protection--&gt;Redis;\n\n    Client--HTTPS--&gt;NGINX</code></pre> <p>and an NGINX config:</p> <pre><code>http {\n    log_format upstreamlog '[$time_local] $remote_addr - $remote_user - $server_name $host to: $upstream_addr: $request $status upstream_response_time $upstream_response_time msec $msec request_time $request_time';\n\n    upstream https_api {\n        server 192.168.1.64:5000;\n        server 192.168.1.68:5000;\n        server 192.168.1.69:5000;\n    }\n\n    upstream web_api {\n        server 192.168.1.60:5000;\n        server 192.168.1.61:5000;\n    }\n\n    server {\n        listen 443 ssl;\n        listen [::]:443 ssl;\n        # The server_name directive specifies cars.be as the domain this server block will respond to, also for /api/!\n        server_name cars.be;\n\n        include snippets/self-signed.conf;\n        include snippets/ssl-params.conf;\n\n        client_header_buffer_size 4k;\n        large_client_header_buffers 4 8k; # Increase if JWT or other headers are large      \n\n        access_log /var/log/nginx/access.log upstreamlog;\n\n        # Serve the website for cars.be except for paths under /api/\n        location / {\n            proxy_pass https://web_api/\n        }\n\n        # The location ^~ /api/ block captures all requests starting with /api/ and forwards them to the https_api upstream. The ^~ modifier gives this location precedence over regular expression locations that might also match.\n        location ^~ /api/ {\n            # The proxy_pass https://https_api; directive tells NGINX to forward the requests to the defined upstream group https_api. Ensure the scheme (https://) matches your upstream configuration. If your upstream servers are configured with HTTP, change the scheme accordingly.\n            proxy_pass https://https_api;\n\n            # Header adjustments are defined inside the /api/ location block to ensure they're applied only to the proxied requests.          \n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_set_header Authorization $http_authorization;\n\n            # Additional necessary proxy settings\n        }\n\n        # The location / block is modified to return a 404 error for any requests that don't match the /api/ path. This effectively drops traffic not intended for the API.        \n        location / {\n            return 404;\n        }\n\n      # Optional: Redirect the root path specifically if necessary\n      # location = / {\n      #     return 404;\n      # }\n\n      # Other possible headers you might want to set\n      # proxy_set_header Authorization \"\"; # If you need to reset the Authorization header\n\n      # WebSocket support (if needed)\n      # proxy_http_version 1.1;\n      # proxy_set_header Upgrade $http_upgrade;\n      # proxy_set_header Connection \"upgrade\";\n    }\n}\n</code></pre> <p>The configuration above now serves a website for requests to cars.be on port 443 with HTTPS, except for HTTPS requests to cars.be/api/, which are proxied to the upstream https_api.  </p> <p>As mentioned, the public certificate may be delivered by Let's Encrypt. If we'd like to automatically request/renew certificates, Let's Encrypt will need to check that we are the owners of our domain (cars.be in this example). Let's Encrypt executes such a test by requesting a special file from the http://domain/.well-known/acme-challenge directory. <code>certbot</code> Is the most common client for Let's Enrypt to generate this special file.  </p> <pre><code>http {\n    log_format upstreamlog '[$time_local] $remote_addr - $remote_user - $server_name $host to: $upstream_addr: $request $status upstream_response_time $upstream_response_time msec $msec request_time $request_time';\n\n    upstream https_api {\n        server 192.168.1.64:5000;\n        server 192.168.1.68:5000;\n        server 192.168.1.69:5000;\n    }\n\n    upstream web_api {\n        server 192.168.1.60:5000;\n        server 192.168.1.61:5000;\n    }\n\n    server {\n        listen 80;\n        server_name cars.be;\n\n        location /.well-known/acme-challenge/ {\n            root /var/www/letsencrypt;\n            allow all;\n        }\n\n        location / {\n            return 301 https://$host$request_uri;\n        }\n    }\n\n    server {\n        listen 443 ssl;\n        listen [::]:443 ssl;\n        # The server_name directive specifies cars.be as the domain this server block will respond to, also for /api/!\n        server_name cars.be;\n\n        include snippets/self-signed.conf;\n        include snippets/ssl-params.conf;\n\n        client_header_buffer_size 4k;\n        large_client_header_buffers 4 8k; # Increase if JWT or other headers are large      \n\n        access_log /var/log/nginx/access.log upstreamlog;\n\n        # Serve the website for cars.be except for paths under /api/\n        location / {\n            proxy_pass https://web_api/;\n        }\n\n        # The location ^~ /api/ block captures all requests starting with /api/ and forwards them to the https_api upstream. The ^~ modifier gives this location precedence over regular expression locations that might also match.\n        location ^~ /api/ {\n            # The proxy_pass https://https_api; directive tells NGINX to forward the requests to the defined upstream group https_api. Ensure the scheme (https://) matches your upstream configuration. If your upstream servers are configured with HTTP, change the scheme accordingly.\n            proxy_pass https://https_api;\n\n            # Header adjustments are defined inside the /api/ location block to ensure they're applied only to the proxied requests.          \n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_set_header Authorization $http_authorization;\n\n            # Additional necessary proxy settings\n        }\n\n      # Other possible headers you might want to set\n      # proxy_set_header Authorization \"\"; # If you need to reset the Authorization header\n\n      # WebSocket support (if needed)\n      # proxy_http_version 1.1;\n      # proxy_set_header Upgrade $http_upgrade;\n      # proxy_set_header Connection \"upgrade\";\n    }\n}\n</code></pre> <p>In the NGINX configuration examples above, the routing of traffic is based on URLs, if we route traffic based on server names we get the following representation:</p> <pre><code>flowchart TB\n    client([Client]) --&gt;|HTTPS request| nginx[NGINX]\n    nginx --&gt;|Decides based on identity.cars.be| identity[NGINX server block &lt;/br&gt; for Authentication]\n    nginx --&gt;|Decides based on abpapi.cars.be| abpapi[NGINX server block &lt;/br&gt; for REST]\n    nginx --&gt;|Decides based on cars.be| web[NGINX server block &lt;/br&gt; for Web]\n\n    subgraph identityZ [ ]\n        identity --&gt; identity_api([Upstream identity_api])\n        identity_api --&gt;|HTTPS traffic| Authentication1\n        identity_api --&gt;|HTTPS traffic| Authentication2\n    end\n\n    subgraph abpapiZ [ ]\n        abpapi --&gt; abp_api([Upstream abp_api])\n        abp_api --&gt;|HTTPS traffic| REST1\n        abp_api --&gt;|HTTPS traffic| REST2\n    end\n\n    subgraph webZ [ ]\n        web --&gt; web_api([Upstream web_api])\n        web_api --&gt;|HTTPS traffic| WEB1\n        web_api --&gt;|HTTPS traffic| WEB2\n    end    </code></pre> <p>For our particular setup, this becomes:</p> <pre><code>events {\n}\n\nhttp {\n\n    client_header_buffer_size 4k;\n    #large_client_header_buffers 4 8k; # Increase if JWT or other headers are large      \n    proxy_buffer_size   128k;\n    proxy_buffers   4 256k;\n    proxy_busy_buffers_size   256k;\n    large_client_header_buffers 4 16k;\n\n    log_format upstreamlog '[$time_local] $remote_addr - $remote_user - $server_name $host to: $upstream_addr: $request $status upstream_response_time $upstream_response_time msec $msec request_time $request_time';\n\n    upstream abp_api {\n        {% for host in groups['http_api'] %}\n        server {{ host }}:{{ hostvars[host]['service_port'] }};\n        {% endfor %}\n    }\n\n    upstream web_api {        \n        {% for host in groups['webserver'] %}\n        server {{ host }}:{{ hostvars[host]['service_port'] }};\n        {% endfor %}\n    }\n\n    upstream identity_api {        \n        {% for host in groups['identity_server'] %}\n        server {{ host }}:{{ hostvars[host]['service_port'] }};\n        {% endfor %}\n    }\n\n    server {\n        listen 443 ssl;\n        listen [::]:443 ssl;\n        # The server_name directive specifies cars.be as the domain this server block will respond to, also for /api/!\n        server_name identity.{{ project_domain }};\n\n        include snippets/self-signed.conf;\n        include snippets/ssl-params.conf;\n\n        access_log /var/log/nginx/access.log upstreamlog;\n\n        # Serve the website for identity.cars.be\n        location / {\n            proxy_pass https://identity_api/;\n\n            # Header adjustments are defined inside the /api/ location block to ensure they're applied only to the proxied requests.          \n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_set_header Authorization $http_authorization;\n\n            # Additional necessary proxy settings\n        }    \n    }\n\n    server {\n        listen 443 ssl;\n        listen [::]:443 ssl;\n        # The server_name directive specifies cars.be as the domain this server block will respond to, also for /api/!\n        server_name abpapi.{{ project_domain }};\n\n        include snippets/self-signed.conf;\n        include snippets/ssl-params.conf;\n\n        access_log /var/log/nginx/access.log upstreamlog;\n\n        # Serve the website for identity.cars.be\n        location / {\n            proxy_pass https://abp_api/;\n\n            # Header adjustments are defined inside the /api/ location block to ensure they're applied only to the proxied requests.          \n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_set_header Authorization $http_authorization;\n\n            # Additional necessary proxy settings\n        }    \n    }\n\n    server {\n        listen 443 ssl;\n        listen [::]:443 ssl;\n        # The server_name directive specifies cars.be as the domain this server block will respond to, also for /api/!\n        server_name {{ project_domain }};\n\n        include snippets/self-signed.conf;\n        include snippets/ssl-params.conf;\n\n        access_log /var/log/nginx/access.log upstreamlog;\n\n        # Serve the website for cars.be except for paths under /api/\n        location / {\n            proxy_pass https://web_api/;\n\n            # Header adjustments are defined inside the /api/ location block to ensure they're applied only to the proxied requests.          \n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_set_header Authorization $http_authorization;\n\n            # Additional necessary proxy settings\n        }        \n\n      # Other possible headers you might want to set\n      # proxy_set_header Authorization \"\"; # If you need to reset the Authorization header\n\n      # WebSocket support (if needed)\n      proxy_http_version 1.1;\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection \"upgrade\";\n    }\n}\n</code></pre> <p>As routing based on hostnames has less impact on the web server and REST endpoint source code (no constraints on URLS), routing based on hostnames is the preferred solution in this project.  </p>"},{"location":"#log-format","title":"Log format","text":"<p>To capture detailed information about requests and responses, we define a custom logging format: the <code>log_format</code> directive specifies a custom format named <code>upstreamlog</code>, which includes timestamps, client IP, the requested server name and host, the upstream server address that handled the request, the request details, status code, response times, and request processing times. This detailed logging is crucial for debugging and monitoring the performance of the upstream servers, in particular inspecting load balancing as the log will display the IP address of the upstream REST API node.</p>"},{"location":"#load-balancing","title":"Load balancing","text":"<p>Distribute incoming traffic across multiple backend servers: the upstream block named <code>http_api</code> defines a cluster of servers (with specified IP addresses and port numbers) that requests will be load balanced across. This setup increases the application's scalability and reliability by distributing the load and providing redundancy.</p>"},{"location":"#https-configuration","title":"HTTPS configuration","text":"<p>We configure NGINX to serve HTTPS traffic and to use specific SSL parameters and certificates.</p> <ul> <li>The listen 443 ssl directives instruct NGINX to listen for incoming connections on port 443 with SSL encryption, including IPv6 connections ([::]:443 ssl).</li> <li>The include directives incorporate additional SSL configurations and self-signed certificate details from external files (self-signed.conf and ssl-params.conf), modularizing the SSL setup for easier management.</li> <li>client_header_buffer_size And large_client_header_buffers directives adjust the buffer sizes for client request headers, accommodating potentially large JWT tokens or other large headers.</li> </ul> <p>Note the HTTPS offloading and re-encryption in the configuration above which involves the NGINX server acting as a termination point for incoming HTTPS connections from clients. NGINX decrypts these connections, inspects the traffic, and then re-encrypts the traffic before forwarding it to the backend servers over HTTPS using separate, internal SSL certificates. This process allows for secure communication both externally with clients and internally within the data center:</p> <ul> <li>Decryption of Incoming HTTPS Requests: NGINX uses public-facing SSL certificates to decrypt data received over SSL/TLS from clients. This is the offloading part.</li> <li>Inspection and Processing: Once decrypted, NGINX can inspect, log, or manipulate the HTTP content as necessary. This step can involve modifying headers, applying access controls, or making routing decisions.</li> <li>Re-Encryption and Forwarding: NGINX then re-encrypts the requests using internal SSL certificates and forwards them to the backend servers over HTTPS. This ensures that traffic remains secure as it traverses the internal network.</li> <li>Secure Internal Communication: The backend servers, configured with the internal certificates, decrypt the re-encrypted requests, process them, and send back the responses. NGINX then encrypts the responses again (if necessary) before sending them back to the clients.</li> </ul> <p>HTTPS offloading and re-encryption has the following benefits:</p> <ul> <li>Enhanced Security Across the Board: This setup ensures end-to-end encryption of data, maintaining security from the client to the NGINX server and from the NGINX server to the backend servers. Using internal certificates for the data center adds an extra layer of security within the internal network.</li> <li>Centralized Public SSL Management: Public-facing SSL certificate management remains centralized at the NGINX server, simplifying the administration of public encryption keys and certificates.</li> <li>Flexible Trust and Security Policies: The separation of external and internal certificates allows for distinct trust domains and security policies. For example, stronger or different encryption standards can be applied internally compared to what is used for public internet traffic.</li> <li>Inspection and Added Controls: Decrypting the traffic at the NGINX layer allows for detailed inspection and the application of additional security controls before re-encrypting and sending it to backend services. This can be crucial for compliance with security policies, logging, or performing deep packet inspection for threat detection.</li> <li>Performance and Scalability: While the NGINX server handles the computational overhead of encrypting and decrypting traffic twice, it offloads backend servers from doing so. This can be optimized by using hardware that accelerates SSL processing. Furthermore, it allows backend servers to be scaled out without the need to manage public SSL certificates on each of them.</li> <li>Secure Internal Traffic: Using HTTPS internally protects against potential threats lurking within the data center, ensuring that sensitive data is encrypted in transit even within the network's trusted boundaries.</li> </ul> <p>In summary, employing HTTPS offloading and re-encryption with NGINX provides robust security by ensuring encrypted traffic both externally and internally while centralizing certificate management and offering the ability to inspect and manipulate HTTP traffic. This configuration enhances the overall security posture and flexibility of managing traffic flows within distributed application architectures.</p>"},{"location":"#access-logging","title":"Access logging","text":"<p>In orde to use a custom-defined log file path and format, the <code>access_log</code> directive specifies the path to the access log file and instructs NGINX to use the upstreamlog format (<code>log_format upstreamlog '[$ti...</code>) for logging, ensuring that detailed information about each request and its handling is logged for future analysis.</p>"},{"location":"#location-block-and-proxy-settings","title":"Location block and proxy settings","text":"<p>Request routing and proxying behavior for the root path is defined with:</p> <ul> <li>The location / block matches all requests and uses the proxy_pass directive to forward them to the http_api upstream cluster, enabling load balancing.</li> <li>The proxy_set_header directives modify request headers to include the original host, the real client IP address, and the protocol used, ensuring that the proxied requests contain necessary information for backend services to process them correctly.</li> <li>The configuration optionally supports forwarding the Authorization header, preserving JWT or other authentication tokens needed by the backend services.</li> </ul>"},{"location":"#optional-websocket-support","title":"Optional websocket support","text":"<p>This part of the configuration provides the necessary headers to support WebSocket connections if needed:</p> <ul> <li>The commented-out directives (<code>proxy_http_version</code>, <code>proxy_set_header Upgrade</code>, and <code>proxy_set_header Connection</code>) are set up to enable WebSocket support, ensuring that NGINX can proxy WebSocket connections correctly. These lines can be uncommented and adjusted as needed based on the application's requirements.</li> </ul>"},{"location":"#references","title":"References","text":"<ul> <li>Playbooks directory</li> <li>Mardown all-in-one</li> </ul>"},{"location":"#todo","title":"TODO","text":"<ul> <li>move proxmox playbook to a different \"folder\"</li> <li>generate the Ansible <code>inventory</code> file or use an <code>inventory</code> file as a vars.yml file to setup VMs (if possible)</li> <li>integrate prometheus alerts or grafana alerts or ...</li> <li>enable tcpdump in a container https://netsplit.uk/posts/2022/10/19/remote_ovh_lab/</li> <li>pfSense <code>pfctl -d</code> and <code>pfctl -e</code> to disable/enable the packet filter</li> <li>add DNS</li> <li>remove cert generation outside of ca/caclient roles</li> </ul>"},{"location":"README_OVH/","title":"Introduction","text":""},{"location":"README_OVH/#ovhhetzner-additional-ip-address","title":"OVH/Hetzner additional IP address","text":"<p>Using an additional IP address and MAC address for a Proxmox VM, instead of the public IP address of the server hosting Proxmox, is often necessary for several reasons related to network configuration, security, and functionality. Here are the key reasons why this approach is used:</p> <ul> <li> <p>Network Isolation and Security</p> <p>Separation of Services: By assigning different IP addresses to different VMs, you can run services on the same ports on each VM without conflicts. This allows for better organization and reduces the risk of accidentally exposing services to the internet. Security Policies: Having separate IP addresses for each VM makes it easier to apply specific firewall and security policies to individual services or applications. This granularity in security management helps in minimizing the potential attack surface.</p> </li> <li> <p>Direct Internet Access</p> </li> <li> <p>Avoiding NAT Complications: Using the host's public IP address typically requires network address translation (NAT) to allow VMs to share that single IP. NAT can complicate setups for certain applications, especially those requiring incoming connections, such as web servers or mail servers. Assigning a unique public IP to each VM simplifies the network configuration by eliminating the need for port forwarding and NAT rules.</p> </li> <li> <p>Performance and Reliability: Directly assigning a public IP to a VM can offer better network performance and reliability for the services running on it, as the traffic does not need to be translated or routed through additional layers.</p> </li> <li> <p>Ease of Migration</p> <p>Flexibility: Having separate IP addresses for VMs makes it easier to migrate them between hosts or data centers with minimal downtime. The VM retains its IP address, ensuring that any DNS configurations or external connections remain intact without the need to update IP addresses in various configurations.</p> </li> <li> <p>MAC Address Binding</p> <p>Unique Network Identity: The MAC address is crucial for network communications at the data link layer. When using additional IP addresses from the provider, binding them to a virtual MAC address ensures that the traffic is correctly routed to and from the VM. This is particularly important in data center environments like OVH or Hetzner, where the network infrastructure relies on MAC addresses to enforce IP allocation and traffic filtering policies.</p> </li> <li> <p>Compliance with Provider Policies</p> <p>IP Allocation Policies: Many cloud and hosting providers, including OVH and Hetzner, have policies that require each public IP address to be associated with a unique MAC address for network management and security purposes. This helps in avoiding IP conflicts and ensuring that each customer's traffic is accurately segregated and routed within the provider's network infrastructure.</p> </li> </ul> <p>In summary, using additional IP and MAC addresses for Proxmox VMs provides better security, network configuration flexibility, direct access to the internet without NAT complications, and compliance with hosting provider policies. This approach offers significant advantages in managing virtualized environments, especially when hosting multiple services or applications that require internet connectivity.</p>"},{"location":"README_OVH/#ovh-additional-ip-address-procedure","title":"OVH additional IP address procedure","text":"<p>To allow a Proxmox VM to access the Internet using an additional IP address you've obtained from OVH, your process involves several steps, including the need to bind the additional IP to a virtual MAC address provided by OVH, and then configuring the Proxmox VM to use this virtual MAC address. Here's a detailed overview of how this process typically works:</p> <ul> <li> <p>Request an Additional IP Address from OVH</p> <p>First, you have to request an additional IP address from OVH for your server. OVH typically allows you to request additional IPs through their control panel. </p> </li> <li> <p>Configure the Proxmox VM with the Virtual MAC Address</p> </li> </ul> <p>Once you have your additional IP and the associated virtual MAC address, you'll need to configure the VM within Proxmox to use this MAC address. Here\u2019s how you can do that:</p> <ul> <li>Access the Proxmox VE interface, and locate the VM you wish to configure.</li> <li>Edit the VM's hardware settings, specifically the network adapter (NIC).</li> <li> <p>Set the MAC address of the VM's network adapter to the virtual MAC address provided by OVH. This ensures that the VM will be recognized on the network with the additional IP address you were assigned. In addition, change the IP and gateway address to the additional IP and gateway address. :fire: You don't have to change the IP address, it is advised to change it to avoid confusion.</p> </li> <li> <p>Configure the VM's Network Settings</p> <p>After setting the virtual MAC address in Proxmox, you'll need to configure the network settings within the VM itself. This involves setting the IP address, subnet mask, gateway, and DNS servers according to the details provided by OVH for your additional IP. The exact steps to do this can vary depending on the operating system of the VM.</p> <p>For Linux VMs, you'll typically edit network configuration files (like /etc/network/interfaces or use nmcli for NetworkManager-based systems). For Windows VMs, you'll go to Network and Sharing Center &gt; Change adapter settings, right-click the network adapter, select Properties, and then configure the IP settings under Internet Protocol Version 4 (TCP/IPv4).</p> </li> <li> <p>Test the Configuration</p> <p>After completing these steps, ensure the VM can access the Internet by performing a simple test, such as pinging an external site (ping google.com) from within the VM. If there are connectivity issues, double-check the MAC address and network configuration settings.</p> </li> </ul>"},{"location":"README_OVH/#proxmox-network-partitioning","title":"Proxmox network partitioning","text":"<p>Partitioning your network for VMs and containers in Proxmox that offer different services involves considerations for scalability, security, manageability, and performance. The optimal network partition strategy depends on several factors, including the size of your environment, the nature of the applications, security requirements, and how the services interact with each other. Here are a few approaches and their implications:</p> <ol> <li> <p>One Subnet per Application</p> <p>Pros:</p> <ul> <li>Isolation: Ensures complete isolation between different applications, improving security by limiting the blast radius in case of a compromise.</li> <li>Simplification: Simplifies network policies and firewall rules specific to each application, making it easier to manage permissions and access control on a per-application basis.</li> </ul> <p>Cons:</p> <ul> <li>Potential Overhead: Could lead to an increase in the number of subnets to manage, especially if you have a large number of small applications.</li> <li>Inter-Application Communication: If applications need to communicate with each other, this approach might complicate networking rules and routing configurations.</li> </ul> </li> <li> <p>One Subnet per Service Type</p> <p>Pros:</p> <ul> <li>Standardization: Groups similar services together, which can simplify the management of network policies for those service types (e.g., all web servers in one subnet).</li> <li>Efficiency in Scaling: Makes scaling a particular service type easier, as new instances can be added to the existing subnet dedicated to that service.</li> </ul> <p>Cons:</p> <ul> <li>Risk of Cross-Service Compromise: If one service is compromised, it could potentially jeopardize other services within the same subnet.</li> <li>Complexity in Inter-Service Communication: Requires careful planning of network routes and firewalls when services in different subnets need to communicate.</li> </ul> </li> <li> <p>Hybrid Approach</p> <p>A hybrid approach combines elements of both strategies. Critical services or those requiring high security might be isolated in their own subnets, while less critical or similar functional services share subnets. This approach allows for a balance between manageability and security.</p> <p>Pros:</p> <ul> <li>Flexibility: Offers a compromise between isolation and manageability, allowing for security policies to be finely tuned based on the risk and interaction between services.</li> <li>Efficient Resource Utilization: Can reduce the number of subnets needed while still maintaining a level of isolation for sensitive components.</li> </ul> <p>Cons:</p> <ul> <li>Complexity: May require more sophisticated network planning and firewall configurations to ensure secure communication across services and applications.</li> </ul> </li> <li> <p>Considerations for VLANs and Firewalls</p> <p>Regardless of the approach, using VLANs can further segment the network physically or logically, adding an additional layer of isolation. Implementing firewalls between subnets (or VLANs) to manage and monitor the traffic flow between services is also critical for securing your network.</p> </li> <li> <p>Cloud-Native Network Policies</p> <p>If you're using containers, especially in a Kubernetes environment, consider leveraging cloud-native network policies for granular control over inter-container communication. These policies can define how groups of pods (a pod is a group of one or more containers) communicate with each other and other network endpoints.</p> </li> </ol> <p>The best approach depends on your specific needs and constraints. A smaller setup might opt for simplicity with fewer subnets, while a larger, security-sensitive environment might require a more complex, hybrid approach for finer-grained control and isolation. In all cases, consider future growth, the potential need for inter-service communication, and the security implications of your network partitioning strategy.</p> <pre><code>graph TD\n    ProxmoxHost(Proxmox Host)\n    ProxmoxHost --&gt;|vmbr0| VM500(PfSense VM)\n    ProxmoxHost --&gt;|vmbr1| VM500\n\n    VM500 --&gt;|eth0| vmbr0[vmbr0&lt;br&gt;Public IPv4: 37.187.28.32&lt;br&gt;Gateway: 37.187.28.254]\n    VM500 --&gt;|eth1| vmbr1[vmbr1&lt;br&gt;LAN Subnet: 192.168.1.1/16&lt;br&gt;Gateway: 192.168.1.51]\n\n    vmbr1 --&gt; C600(Web Server)\n    vmbr1 --&gt; C601(Redis Server)\n    vmbr1 --&gt; C602(Database Server)\n    vmbr1 --&gt; C603(Authentication Server)\n    vmbr1 --&gt; C605(Grafana Server)\n    vmbr1 --&gt; C606(Prometheus Server)\n    vmbr1 --&gt; C607(OTLP Server)\n    vmbr1 --&gt; C610(Nginx Server)\n\n    C610 --&gt; C604(Rest Endpoint 1)\n    C610 --&gt; C608(Rest Endpoint 2)\n    C610 --&gt; C609(Rest Endpoint 3)\n\n</code></pre>"},{"location":"docs/","title":"Welcome to hostr","text":"<p>This documentation is generated with MkDocs. For full documentation visit mkdocs.org.</p>"},{"location":"docs/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul> <p>If you installed with pip, you can find mkdocs at <code>/home/user_name/.local/bin/</code></p>"},{"location":"docs/#installation","title":"Installation","text":"<p>Install <code>MKDocs</code>:</p> <pre><code>pip install mkdocs\n</code></pre> <p><code>MkDocs</code> does not allow re-using your <code>README.md</code> in your root folder, install the following plugin to allow <code>mkdocs.yml</code> in the same folder as your documentation (the <code>docs</code> folder which normally contains your documentation):</p> <pre><code>pip install mkdocs-same-dir\n</code></pre> <p><code>Material for MkDocs</code> Started as a stylesheet for <code>MkDocs</code> and is now a powerful documentation framework on top of <code>MkDocs</code>, install with:</p> <pre><code>pip install mkdocs-material\n</code></pre> <p>and add the following line to <code>mkdocs.yml</code>:</p> <pre><code>theme:\n  name: material\n\nmarkdown_extensions:\n  - pymdownx.superfences:\n      custom_fences:\n        - name: mermaid\n          class: mermaid\n          format: !!python/name:pymdownx.superfences.fence_code_format\n</code></pre> <p>Typically, you can find the configuration settings in a configuration section on the Material for MkDocs documentation site.  </p> <p>Check the mkdocs file for more plugin/extension examples.</p>"},{"location":"docs/#project-layout","title":"Project layout","text":"<pre><code>    mkdocs.yml    # The configuration file.\n    docs/\n        index.md  # The documentation homepage.\n        ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"docs/about/","title":"About hostr","text":"<p>Developed by Bart over the weekends. </p>"},{"location":"docs/applicationperformancemonitoring/","title":"Application Performance Monitoring","text":"<p>In this project we will use Prometheus as it supports OTLP (Open Telemetry Protocol) which is used in the example code (web site and REST API). You can also export OTel (Open Telemetry) data to Jaeger or Zipkin. For an overview, please check the following list of  APM (Application Performance Monitoring) vendors.</p> <pre><code>flowchart TD;    \n    WebServer[Web Server]-- \":5000/metrics\" --&gt;Prometheus;\n    RESTAPI[REST API]-- \":5000/metrics\" --&gt;Prometheus;\n    Prometheus-- \":9090\" --&gt;Grafana</code></pre> <p>:fire: The example code is written in dotnet core, which has its own instrumentation APIs for logging, metrics and tracing. Therefore, OTel collects telemetry from the build in dotnet core APIs. The benefit that OTel brings as an industry standard is a common mechanism for collecting telemetry data and integrating APMs. For more information see.</p> <p>:fire: If you are using a different programming language, please check the supported programming languages.  </p>"},{"location":"docs/applicationperformancemonitoring/#prometheus-server","title":"Prometheus server","text":"<p>As illustrated in the figure above, Prometheus is configured with the metrics endpoints of the Web Server and REST API. Extend this configuration if you'd like to inlude other systems/microservices/etc.</p>"},{"location":"docs/applicationperformancemonitoring/#install-prometheus-server","title":"Install Prometheus server","text":"<pre><code>ansible-playbook prometheus.yml -i hosts -K -vvv\n</code></pre>"},{"location":"docs/applicationperformancemonitoring/#collector","title":"Collector","text":"<p>For a more flexible setup, you may be interested in the setup of an Otel collector. For example <code>InfluxDB</code> with Telegraf and the OpenTelemetry Input Plugin:</p> <pre><code>flowchart TD;    \n    WebServer[Web Server]-- \"OTLP GRPC\" --&gt;Telegraf;\n    RESTAPI[REST API]-- \"OTLP GRPC\" --&gt;Telegraf;\n    Telegraf----&gt;InfluxDB;\n    Telegraf----&gt;Kapacitor;\n    Kapacitor&lt;----&gt;Chronograf\n    Kapacitor&lt;----&gt;InfluxDB;\n    InfluxDB----&gt;Grafana;\n    InfluxDB----&gt;Zipkin/Prometheus/...;\n    InfluxDB&lt;----&gt;Chronograf;</code></pre> <p>For completeness, the diagram above mentions Chronograf the user interface and administrative comonent of InfluxDB which allows to interact with the TICK-stack and Zipkin a distributed tracing system.  </p> <p>You can also use the OpenTelemetry Collector.  </p>"},{"location":"docs/applicationperformancemonitoring/#install-influxdb","title":"Install InfluxDB","text":"<pre><code>ansible-playbook otelcollector.yml -i hosts -K -vvv\n</code></pre> <p>After installing, open the UI on <code>http://{{ groups['otlp_controller'][0] }}:8086)</code>.</p>"},{"location":"docs/applicationperformancemonitoring/#grafana-server","title":"Grafana server","text":""},{"location":"docs/applicationperformancemonitoring/#install-grafana-server","title":"Install Grafana server","text":"<pre><code>ansible-playbook grafana.yml -i hosts -K -vvv\n</code></pre> <p>After installing Grafana, you can browse to the Grafana endpoint (check the Grafana endpoint in hosts) and open the <code>ASP .NET Core</code> dashboard: </p> <p></p>"},{"location":"docs/authenticationserver/","title":"Authentication Server","text":""},{"location":"docs/authenticationserver/#install-authentication-server","title":"Install Authentication Server","text":"<p><code>ABP</code> supports IdentityServer4 and OpenIddict. As the <code>ABP</code> startup templates support OpenIddict since ABP v6.0.0 we will only support <code>OpenIddict</code>.  </p> <pre><code>ansible-playbook authentication.yml -i hosts -K -vvv\n</code></pre> <p>After installing the authentication server, you can request a client credentials (server to server), check the documentation and example in the cars.be repo.</p>"},{"location":"docs/certificateauthority/","title":"Certificate Authority","text":"<p>Setting up a CA chain with Ansible and automating the process of generating CSRs (Certificate Signing Requests) for each virtual machine/container is a robust and secure way to manage certificates in our infrastructure. For developers, it is probably sufficient to generate certificates used to encrypt internal traffic and for your public endpoint in your LAN. If you'd like to publish your public endpoint on the Internet, you should use a CA like Let's Encrypt.  </p> <ol> <li> <p>Ansible Roles:</p> <ul> <li>CA Role: This role sets up a Certificate Authority (CA).</li> <li>Client Role: This role will generate CSRs for each virtual machine and handle the signing of certificates by the CA.</li> </ul> </li> <li> <p>CA Role:</p> <ul> <li>Configure the CA server: Install and configure software like OpenSSL to act as the CA.</li> <li>Generate the CA certificate and private key.</li> <li>Create necessary directories to store certificates and keys.</li> <li>Set up appropriate permissions to ensure security.</li> </ul> </li> <li> <p>Client Role:</p> <ul> <li>Install necessary tools like OpenSSL for generating CSRs.</li> <li>Generate CSRs for each virtual machine.</li> <li>Securely transmit the CSRs to the CA server.</li> <li>Sign the CSRs.</li> <li>Retrieve signed certificates from the CA.</li> </ul> </li> </ol> <p>By following the steps above, we can set up our own (small/simple) CA chain with Ansible and automate the process of generating CSRs and signing certificates for our virtual machines/internal traffic.</p> <p>To ensure that consumers trust our client certificates and/or to establish secure communication channels within our infrastructure (network communication within the datacentre), we have several options:</p> <ul> <li>Distribute CA Certificate: Distribute the CA certificate (which was used to sign the client certificates) to the consumers of the client certificates. Consumers can then import the CA certificate into their trust store (e.g., browser's certificate store, operating system's certificate store) as a trusted root CA. This allows the consumers to trust any certificates signed by that CA. :cupid: If you run the Ansible playbooks on Ubuntu, the <code>caclient</code> playbook will install the root certificate for you, on Windows, you will have to install the certificate manually (at least, for now).</li> <li>Certificate Chain: Along with the client certificate, provide the entire certificate chain up to the root CA certificate. This includes the client certificate, any intermediate CA certificates, and the root CA certificate. Consumers can then verify the certificate chain, ensuring that each certificate in the chain is signed by the preceding one, up to the trusted root CA certificate. :fire: In this particular implementation, we have no certificate chain.</li> <li>Publicly Trusted CA: If you require wider trust, consider obtaining client certificates from a publicly trusted CA (e.g., Let's Encrypt, DigiCert). Certificates issued by publicly trusted CAs are automatically trusted by most systems, as they are included in the trust stores of popular browsers and operating systems. :fire: For our public endpoint, we will use Let's Encrypt, internal traffic, however, will be encrypted with the self-generated client certificates.</li> <li>Custom Trust Store: Create a custom trust store containing the CA certificate or certificate chain and distribute it to the consumers of the client certificates. Applications can then be configured to use this custom trust store to verify the authenticity of client certificates. :fire: Might be used in a future implementation.</li> <li>Certificate Pinning: Implement certificate pinning, where consumers explicitly trust a specific client certificate or CA certificate by embedding its fingerprint or public key within the application code. This ensures that only the specified certificate or CA is trusted, providing an additional layer of security. :fire: Might be used in a future implementation.</li> <li>Education and Documentation: Educate consumers about the trust model and security practices involved in certificate authentication. Provide clear documentation on how to verify and trust client certificates, including instructions for importing CA certificates or certificate chains into trust stores. :fire: Might be used in a future implementation.</li> </ul> <p>In our implementation, the Client Role will copy the CA certificate to the default Ubuntu certificate stores.</p>"},{"location":"docs/certificateauthority/#install-certificate-authority","title":"Install Certificate Authority","text":"<pre><code>ansible-playbook ca.yml -i hosts -K -vvv\n</code></pre>"},{"location":"docs/certificateauthority/#generate-client-certificates","title":"Generate Client Certificates","text":"<pre><code>ansible-playbook caclient.yml -i hosts -K -vvv\n</code></pre>"},{"location":"docs/contributing/","title":"Contributing","text":"<p>We welcome contributions to improve the scripts or documentation. Please feel free to fork the repository, make your changes, and submit a pull request.</p>"},{"location":"docs/database/","title":"Database","text":""},{"location":"docs/database/#install-sql-server","title":"Install SQL Server","text":"<pre><code>ansible-playbook mssql.yml -i hosts -K -vvv\n</code></pre> <p>After installing your DB server, you can SSH into the machine and open an SQL shell with (assuming you use password <code>p@55w0rD</code>):</p> <pre><code>sqlcmd -S 127.0.0.1 -U sa -P p@55w0rD -C\n</code></pre>"},{"location":"docs/detailedprerequisites/","title":"Detailed Prerequisites","text":""},{"location":"docs/detailedprerequisites/#proxmox-prerequisites","title":"Proxmox prerequisites","text":"<p>If you'd like to deploy on Proxmox, first install Proxmox on a host. Your host is a bare metal machine or a virtual machine with nesting enabled. Qemu For instance, allows to nest VMs. TBC </p> <p>Depending on your particular installation and/or test requirements, it may be relevant to configure an DNS server during the Proxmox installation: this will ensure that the machine running Proxmox (bare metal or VM) and the VMs running on Proxmox all are in the same subnet.</p> <p>In this document/config it is assumed that you will use the following domain(s):</p> <pre><code># the ip address of the machine that (will) run(s) your web site, for example the ip address of proxmox vm 100\n192.168.1.60 cars.be\n</code></pre> <p>Add the entries above in the hosts file on</p> <ul> <li>the machine running the Ansible installation scripts</li> <li>the clients you use to visit the web site</li> </ul> <p>If all has gone well, you should have (after running the Ansible scripts) a setup like:</p> <p></p>"},{"location":"docs/detailedprerequisites/#hyper-v-install","title":"Hyper-V install","text":"<p>You can install Proxmox on Windows Hyper-V. Create the virtual machine with the standard procedure and execute the following command in Powershell</p> <pre><code>Set-VMProcessor -VMName &lt;Name of your Proxmox VM&gt; -ExposeVirtualizationExtensions $true\n</code></pre> <p>to switch on Hyper-V's nested virtualization. Download and mount the Proxmox ISO, after booting, click in the VM screen and tap on Enter to start the install. If your install hangs at <code>Trying to detect country</code>, disable your internet connection and restart the installer.  </p> <p>After installing Proxmox on Hyper-V, you will notice that calling an endpoint on your Hyper-V VM from WSL2 doesn't work. Open the Hyper-V Manager and next the Virtual Switch Manager</p> <p></p> <p>and note the two different virtual switches. As documented on we need to enable forwarding across both virtual switches:</p> <pre><code>Get-NetIPInterface | where {$_.InterfaceAlias -eq 'vEthernet (Default Switch)' -or $_.InterfaceAlias -eq 'vEthernet (WSL (Hyper-V firewall))'} | Set-NetIPInterface -Forwarding Enabled\n</code></pre> <p>Check setting forwarding with:</p> <pre><code>Get-NetIPInterface | select ifIndex,InterfaceAlias,AddressFamily,ConnectionState,Forwarding | Sort-Object -Property IfIndex | Format-Table\n</code></pre> <p>:fire: Note the vEthernet(...) in the IP interface aliases, if you are not sure about the aliases, first run</p> <pre><code>Get-NetIPInterface\n</code></pre> <p>:fire: This setting is disabled after rebooting!</p> <p>and check the second column.  </p> <p>Besides forwarding traffic between the (virtual) switches, you can also add a route which routes traffic destined for your Proxmox instance to the virtual switch used by your VM. In the network settings of your VM, you will have to use the IP address of your virtual switch as the gateway.</p> <p>For example, if the IP address of your VM is <code>172.18.60.44</code> and the IP address of your Hyper-V virtual switch used by your VM is <code>172.17.240.1</code>, you can route the traffic destined for your VM with:  </p> <pre><code>route -p add 172.18.60.44 MASK 255.255.255.255 172.17.240.1\n</code></pre> <p>the <code>-p</code> option ensures that the route will persist after a reboot. The <code>MASK</code> ensures that only the traffic for the VM is routed over the virtual switch. If you run several VMs, you may have to change the <code>MASK</code>.  </p> <p>Later on, you can remove this route with:</p> <pre><code>route -p delete 172.18.60.44\n</code></pre> <p>You can get the IP address of your VM by executing the following command on your VM (open a VM connection in the Hyper-V Manager):</p> <pre><code>ip addr\n</code></pre> <p>and you get the IP address of your virtual switch by executing the following command on the Windows host:  </p> <pre><code>ipconfig\n</code></pre> <p>Look for the IP address of a switch with a name like <code>Ethernet adapter vEthernet (Default Switch)</code>.  </p> <p>:fire: Note that the host (the machine running Hyper-V) may change its IP address, it is the IP address of the VM and the IP address of the switch which have to be static in this setup.  </p> <p>After adding the route, you will need to change the gateway on your VM as the route defines how to forward traffic to the VM, the path for the return traffic (the replies) will be defined by the gateway. For Proxmox edit the following file on the VM:  </p> <pre><code>nano /etc/networks/interfaces\n</code></pre> <p>and change the gateway in the interfaces config, for example:  </p> <pre><code># network interface settings; autogenerated\n# Please do NOT modify this file directly, unless you know what\n# you're doing.\n#\n# If you want to manage parts of the network configuration manually,\n# please utilize the 'source' or 'source-directory' directives to do\n# so.\n# PVE will preserve these directives, but will NOT read its network\n# configuration from sourced files, so do not attempt to move any of\n# the PVE managed interfaces into external files!\n\nauto lo\niface lo inet loopback\n\niface eth0 inet manual\n\nauto vmbr0\niface vmbr0 inet static\n        address 172.18.60.44/16\n        #gateway 172.18.60.1\n        gateway 172.17.240.1 # the ip address of your virtual switch\n        bridge-ports eth0\n        bridge-stp off\n        bridge-fd 0\n</code></pre> <p>Another option is setting up a <code>NAT</code>. As routing probably has the least impact, using routing is probably wise:  </p> <ul> <li> <p>Minimal Configuration Changes: Unlike reconfiguring network settings on either the host or the VM, routing typically involves making changes at the network infrastructure level. Once set up, these routing configurations generally require minimal maintenance and do not impact the individual devices' network settings.</p> </li> <li> <p>Supports Dynamic IP Assignment: Routing can accommodate scenarios where devices may obtain IP addresses dynamically, such as through DHCP. As long as routing configurations are properly set up, devices can communicate regardless of their assigned IP addresses.</p> </li> <li> <p>Scalability: Routing allows for scalability and flexibility. It enables communication between devices across different subnets, in our case communication between the subnet of the Windows host and the subnet of the VM.</p> </li> <li> <p>Isolation: Routing helps maintain network isolation between different segments, this is probably less relevant for this setup.</p> </li> <li> <p>Centralized Management: Routing configurations are typically managed centrally, similar to isolation, less relevant for this setup.</p> </li> </ul>"},{"location":"docs/detailedprerequisites/#bare-metal-install","title":"Bare metal install","text":"<p>Follow the default installation procedure on.</p>"},{"location":"docs/detailedprerequisites/#promox-api-user","title":"Promox API user","text":"<p>Before using the Ansible scripts, you will need a Proxmox API user:</p> <ul> <li>Log into the Proxmox Web UI</li> <li>Select datacenter from the left menu</li> <li>Select users fromf the left sub-menu</li> <li>We will use the default root@pam user</li> <li>Select API tokens from the left sub-menu</li> <li>Click add</li> <li>User: root@pam</li> <li>Token ID: input a random string, for example and record the token ID</li> <li>record the token secret</li> </ul> <p>After adding the user and API token, select Datacenter and click on Permissions and Add API Token Permission. Choose</p> <ul> <li>Path: /</li> <li>API Token: root@pam <li>Role: Administrator</li> <li>Propagate: select</li>"},{"location":"docs/detailedprerequisites/#ansible-prerequisites","title":"Ansible prerequisites","text":"<p>As we are using Ansible to automate our install and deploy, you will have to meet the following minimum requirements on the hosts which will run the Ansible scripts: Ansible minimum requirements.</p> <p>For this project, we will need some extra dependencies. Install the following Ansible role(s):</p> <pre><code>ansible-galaxy install geerlingguy.redis\n</code></pre> <p>:fire: Microsoft has an ansible SQL Server role which supports RHEL, as we use Ubuntu for now, the Microsoft Ansible SQL Server role is not usable.</p> <p>or write all dependencies to a <code>requirements.yml</code> file</p> <pre><code>---\nroles:\n  - name: geerlingguy.redis\n    type: galaxy\n    source: https://galaxy.ansible.com\n\ncollections:\n  # With just the collection name\n  - my_namespace.my_collection    \n</code></pre> <p>and install with:</p> <pre><code>ansible-galaxy role install -r requirements.yml\nansible-galaxy collection install -r requirements.yml\n</code></pre> <p>Add additional Python dependencies with:</p> <pre><code>pip install proxmoxer\n</code></pre>"},{"location":"docs/detailedprerequisites/#secrets-prerequisites","title":"Secrets prerequisites","text":"<p>Secrets are stored in env variables in files</p> <ul> <li>.env</li> <li>.env.prod</li> <li>.env.acc</li> <li>.env.dev</li> </ul> <p>Keep your PROD, ACC and DEV secrets in the corresponding files and copy to <code>.env</code> before you start the ansible scripts. <code>source</code> .env Before executing your first Ansible script.</p> <p>:fire: Obviously, none of the <code>env</code> files belongs in your repo.</p> <p>Your <code>env</code> file should at least contain your public key to connect with ssh to the VMs:</p> <pre><code>#!/bin/bash\nexport SSH_PUB_KEY=\"ssh-rsa AAAAB.... your_name@your_machine\"\n</code></pre> <p>If you are unsure about your public key, check <code>~/.ssh/id_rsa_pub</code>.</p>"},{"location":"docs/detailedprerequisites/#ansible-role-settings","title":"Ansible role settings","text":"<p>Each role has its variables (settings) defind in <code>role_folder/defaults/main.yml</code>. Variables are read from env variables:</p> <pre><code>project_name: \"{{ lookup('ansible.builtin.env', 'PROJECT_NAME') }}\"\n</code></pre> <p>You can - as usual in Ansible - easily override the default variables.  </p>"},{"location":"docs/detailedprerequisites/#gitssh-prerequisites","title":"Git/SSH prerequisites","text":"<p>When managing source code across virtual machines (VMs) on platforms like Proxmox, accessing source code from version control systems may have a security impact. A common practice involves cloning repositories using Git over SSH, authenticated via private SSH keys. However, storing private keys directly on VMs, especially those hosted on third-party servers, raises potential significant security risks. Instead, SSH agent forwarding emerges as a safer alternative.  </p> <p>SSH agent forwarding allows users to access a remote machine through SSH without placing private SSH keys on the server itself. Instead, the SSH authentication request is forwarded back to the user's local machine where the SSH agent resides. This means the private key is never exposed to the network or stored on the remote server, obviously enhancing security.</p> <p>You can setup SSH agent forwarding on your local machine with:</p> <ul> <li>Enable SSH Agent on Local Machine: Before initiating a connection, ensure the SSH agent is running on your local machine and your private key is added. This can be done using:</li> </ul> <pre><code>eval $(ssh-agent -s)  \n# note the full path, if you are using WSL and the windows agent (1password, etc.) you have to be explicit about \"which ssh config\" you are changing (Ansible does not use the 1password Windows agent)\n/usr/bin/ssh-add # add all keys under ~/.ssh/\n/usr/bin/ssh-add -l # list all keys\n</code></pre> <ul> <li>Configure SSH to Use Agent Forwarding: Modify the SSH configuration file (~/.ssh/config) on your local machine to enable agent forwarding. You can specify this globally or for specific hosts:</li> </ul> <pre><code>Host 192.168.1.*    \n  ForwardAgent yes\n</code></pre> <ul> <li>Connect to the VM: Once configured, you can connect to the VM using SSH. The agent forwarding will carry your authentication from the local machine to the VM:</li> </ul> <pre><code>/usr/bin/ssh-add # note the full path, if you are using WSL and the windows agent (1password, etc.) you have to be explicit about \"which ssh config\" you are changing (Ansible does not use the 1password Windows agent)\n/usr/bin/ssh -v -T git@github.com\n#Hi your_account! You've successfully authenticated, but GitHub does not provide shell access.\n/usr/bin/ssh root@192.168.1.60 # SSH into your host, you can test this once your hosts are \"online and reachable\"\necho $SSH_AUTH_SOCK #response should not be empty\n#/tmp/ssh-n9uZGGjGAC/agent.2806\nssh -v -T git@github.com # on your host, you probably are not using WSL and the windows agent  \n#Hi username You've successfully authenticated, but GitHub does not provide shell access.\n</code></pre> <ul> <li>Clone Repository Using SSH on the VM: On the VM, use Git to clone the repository as usual. The SSH agent will authenticate using your local machine\u2019s private key, as an example (will be executed in Ansible scripts):  </li> </ul> <pre><code>git clone git@github.com:user/repository.git\n</code></pre> <p>Some advantages of SSH agent forwarding:</p> <ul> <li>Enhanced Security: The primary advantage is security. Your private SSH keys remain secure on your local machine, reducing the risk of exposure through VM compromise.</li> <li>Ease of Use: Once set up, the process is transparent to the user. You can seamlessly authenticate to Git repositories without managing multiple keys on different VMs.</li> </ul> <p>Some drawbacks and potential considerations:</p> <ul> <li>Security Risks with Misconfiguration: If not configured properly, SSH agent forwarding can expose your local SSH agent to remote machines, which could be a vector for an attack if the remote machine is compromised.</li> <li>Dependency on Local Machine: Your local machine must be online and accessible whenever operations requiring SSH authentication are performed, potentially limiting flexibility.</li> <li>Complex Setup: The initial setup requires understanding of SSH configurations and might be more complex compared to simply copying keys to servers.</li> </ul> <p>Some best practices:</p> <ul> <li>Limit Forwarding: Only enable agent forwarding to trusted hosts to mitigate the risk of exposing your SSH agent to a compromised server.</li> <li>Monitor Sessions: Keep an eye on active SSH sessions that utilize agent forwarding to ensure they are legitimate and necessary.</li> <li>Security Hardening: Regularly update and patch both local and remote systems to protect against vulnerabilities.</li> </ul> <p>:fire: If your source code is hosted on a public repository, you can off course skip the steps above.  </p>"},{"location":"docs/detailedprerequisites/#proxmox-firewall-preparation","title":"Proxmox Firewall Preparation","text":"<p>Before you can install a firewall, you have to make sure that at least one of your VMs an access the Internet, all other VMs on Proxmox have to be able to communicate with each other in a LAN only. No Internet connectivity is required for the \"non firewall\" VMs.  </p> <p>In order to arrange this setup, we will need two bridges:</p> <ul> <li>one bridge which will bridge the incoming Internet traffic to the firewall and vice-versa</li> <li>one bridge which will allow the VMs to communicate within a LAN</li> <li>the VMs within the LAN use the LAN IP address of the firewall as gateway to return traffic over the firewall/VPN</li> </ul> <p>A step-by-step guide to achieve this:</p> <ul> <li>You should have one existing bridge with a IPv4 which equals your public IP address (not the additional IP address)</li> <li>edit the MAC address of the network interface of the VM which has to access the Internet, enter the MAC address of the additional IP address</li> <li>Create a New Bridge for the Internal Network</li> <li>Log into Proxmox VE Web Interface: Open your browser and access the Proxmox VE web interface by navigating to your Proxmox server's IP address.  </li> <li>Navigate to 'System' -&gt; 'Network': Here you will see your current network configuration, including any bridges and physical interfaces.</li> <li>Create a New Bridge:<ul> <li>Click on \"Create\" and select \"Linux Bridge\".</li> <li>Give the bridge a meaningful name, like vmbr1 (assuming vmbr0 is your existing bridge for the internet connection).</li> <li>Assign it an IP address that fits within your internal network scheme (e.g., 192.168.1.1/16). If the Proxmox host doesn't need to communicate on this network, you may leave the IP address field empty.</li> <li>Leave the Gateway field empty (the existing bridge has a Gateway)</li> <li>Ensure the \"Autostart\" option is checked.</li> <li>Leave the \"Bridge ports\" field empty if this bridge is for internal VM communication only.</li> <li>Click \"Create\".</li> </ul> </li> <li>Attach a Second Network Interface to Your VM</li> <li>Select Your VM: In the Proxmox web interface, go to the \"Server View\", find the VM you want to configure, and click on it.</li> <li>Add Network Device:<ul> <li>With the VM selected, go to the \"Hardware\" tab.</li> <li>Click \"Add\" and choose \"Network Device\".</li> <li>For \"Model\", you can choose \"VirtIO (paravirtualized)\" for better performance or \"e1000\" for broader compatibility.</li> <li>Under \"Bridge\", select the new bridge you created (vmbr1).</li> <li>Click \"Add\".</li> </ul> </li> <li>Configure the Operating System Inside the VM: After attaching the second network interface, you need to configure the operating system inside the VM to use this new interface.  </li> <li>Access Your VM: Log into your VM via the Proxmox console or SSH.</li> <li>Identify the New Interface: Run ip a or ifconfig to list your network interfaces. You should see a new one (likely named eth1 or similar).</li> <li>Configure the Network Interface: You'll need to edit the network configuration file or use a network manager to configure the interface. The exact steps vary depending on your Linux distribution, for Ubuntu:</li> </ul> <pre><code># /etc/network/interfaces - create the file if it does not exist\nauto eth1\niface eth1 inet static\n    address 192.168.1.x\n    netmask 255.255.0.0\n    gateway 192.168.1.1\n</code></pre> <p>For pfSense, you don't need to edit /etc/network/interfaces or /etc/netplan/..., it is sufficient to add the second network interface and assign its IP address in the pfSense UI. * Restart the Network Service or the VM to apply changes.  </p>"},{"location":"docs/detailedprerequisites/#bridges","title":"Bridges","text":"<p>The terms bridge and switch are often used interchangeably in the context of networking, but they refer to devices that operate at the data link layer (Layer 2) of the OSI model.  </p> <p>A bridge in Proxmox (or any other virtualization platform) is conceptually closer to a physical switch than to a traditional physical bridge in terms of its functionality and use case. Here's why:</p> <ul> <li> <p>Virtual Bridges and Physical Switches: Similarities</p> </li> <li> <p>Multiple Connections: Both virtual bridges in Proxmox and physical switches are designed to connect multiple devices (or virtual machines/containers in the case of Proxmox) within a network. They allow for the creation of network segments that can communicate internally and externally.</p> </li> <li>Traffic Management: Like a physical switch, a virtual bridge can manage and forward traffic between its connected interfaces based on MAC addresses, efficiently directing packets to their intended destinations within the virtual network or to external networks. </li> <li> <p>Advanced Networking Features: While not as feature-rich as some high-end physical switches, virtual bridges in Proxmox can still offer several advanced networking features, such as VLAN tagging, which is a hallmark of switch capabilities.</p> </li> <li> <p>Differences from Physical Bridges</p> </li> <li> <p>Functionality: Traditional physical bridges were primarily used to connect two network segments, with a focus on reducing collision domains and managing traffic between these segments. While a Proxmox bridge can perform a similar role in a virtual environment, it more closely mirrors the multiport, multipurpose functionality of switches by allowing numerous virtual machines and containers to connect to various networks.</p> </li> <li> <p>Port Density: Physical bridges typically have a very limited number of ports (often just two), akin to the simplest form of segmentation. In contrast, a virtual bridge in Proxmox can handle connections from numerous VMs and containers simultaneously, much like a physical switch with many ports.</p> </li> <li> <p>Use in Virtualization</p> </li> <li> <p>Network Virtualization: Virtual bridges play a crucial role in network virtualization, providing a platform for VMs and containers to communicate as if they were connected to a physical switch. This is essential for creating complex virtual network topologies that resemble physical network infrastructures.</p> </li> <li>Flexibility and Scalability: In virtual environments, bridges offer a level of flexibility and scalability that is more characteristic of switches. Administrators can dynamically adjust network configurations, add or remove VMs from networks, and implement security policies or VLANs without needing physical hardware changes.</li> </ul> <p>In summary, while the terminology may suggest a direct analogy to physical bridges, in practice, the role of a bridge in Proxmox and other virtualized environments aligns more closely with the functionalities of a physical switch, especially concerning its ability to connect multiple devices and manage network traffic efficiently within a virtualized networking context.</p> <p>For our setup, that becomes:</p> <pre><code>graph TB\n  subgraph Internet\n  end\n  subgraph Proxmox\n    direction TB\n\n    subgraph vmbr0 [vmbr0&lt;br&gt;Public IPv4: 37.187.28.32&lt;br&gt;Gateway: 37.187.28.254]\n\n    end\n\n    subgraph VM500 [VM 500 - PfSense]\n      direction TB\n      eth0(eth0&lt;br&gt;MAC Additional IP)\n      eth1(eth1&lt;br&gt;IP: 192.168.1.51)\n      eth0 &lt;--firewall--&gt; eth1\n    end\n\n    subgraph vmbr1 [vmbr1&lt;br&gt;LAN Subnet: 192.168.1.1/16&lt;br&gt;Gateway: 192.168.1.51]\n\n    end       \n\n    subgraph LAN [LAN Subnet: 192.168.1.1/16&lt;br&gt;Gateway: 192.168.1.51]\n\n    end\n\n    vmbr0 --&gt; eth0\n    eth1 --&gt; vmbr1\n    vmbr1 --&gt; LAN    \n  end \n  Internet &lt;---&gt; vmbr0  </code></pre>"},{"location":"docs/httpsoffloading/","title":"HTTPS Offloading","text":"<p>In the previous section, we explained how certificates are generated with our own CA. We can now use the certificates to encrypt internal traffic after offloading HTTPS traffic.</p> <p>As we'd like to use HTTPS offloading and load balance the web server and REST endpoint, we migth choose a setup similar to:</p> <pre><code>graph TB\n    client1(Internet Client 1) --&gt;|HTTPS request| nginx[Load Balancing&lt;br/&gt;HTTPS offloading]\n    client2(Internet Client 2) --&gt;|HTTPS request| nginx\n    client3(Internet Client 3) --&gt;|HTTPS request| nginx\n\n    nginx --&gt;|Distribute request| ws1[Web Server 1]\n    nginx --&gt;|Distribute request| ws2[Web Server 2]\n    nginx --&gt;|Distribute request| re1[REST Endpoint 1]\n    nginx --&gt;|Distribute request| re2[REST Endpoint 2]\n\n    ws1 --&gt;|Optional state management| redis[(REDIS Instance)]\n    ws2 --&gt;|Optional state management| redis\n    re1 --&gt;|Optional state management| redis\n    re2 --&gt;|Optional state management| redis</code></pre> <p>Note that internal traffic (for example the Web Server querying the REST endpoint) runs over a publically acessible proxy and is not necessarily encrypted. Moreover, if the proxy acting as the HTTPS offloader and load balancer is compromised, an attacker could potentially gain access to both the unencrypted internal traffic routed through it and the encrypted traffic it decrypts, which makes it a critical point of security.  </p> <p>Implementing a Demilitarized Zone (DMZ) is an approach to solve this: mitigate risks by segregating different parts of a network, hence, limiting the potential impact of a breach. A potential approach:  </p> <pre><code>graph TD\n    subgraph Internet\n      client[Client]\n    end\n\n    subgraph DMZ\n      nginx_public[ProxyPublic]\n    end\n\n    subgraph Internal Network\n      nginx_internal[ProxyInternal]\n      rest[REST Endpoints]\n      web[Web Servers]\n      redis[(REDIS)]\n    end\n\n    client --&gt; nginx_public\n    nginx_public -.-&gt;|Secure Channel| nginx_internal\n    nginx_internal --&gt; rest\n    nginx_internal --&gt; web\n    rest --&gt;|State Management| redis\n    web --&gt;|State Management| redis</code></pre> <p>From the diagram above, the segregation of both networks should be obvious: internal traffic is routed over <code>ProxyInternal</code> and not visible on (a breached) <code>ProxyPublic</code>.  </p> <p>As the proxy acts as the entry point for incoming traffic, it can offload SSL/TLS for incoming HTTPS requests. The proxy decrypts the HTTPS traffic, re-encrypts the traffic and then routes the requests to the appropriate backend services. Backend services can - for instance - use certificates from an internal Certificate Authority (CA).  </p> <pre><code>graph TD\n    subgraph Internet\n      client[Client]\n    end\n\n    subgraph DMZ\n      nginx_public[ProxyPublic]\n    end\n\n    subgraph Internal Network\n      nginx_internal[ProxyInternal]\n      rest[REST Endpoints]\n      web[Web Servers]\n      redis[(REDIS)]\n    end\n\n    client --&gt;|HTTPS| nginx_public    \n    nginx_public --&gt;|HTTPS| nginx_internal\n    nginx_internal --&gt;|HTTPS| rest\n    nginx_internal --&gt;|HTTPS| web\n    rest --&gt;|State Management| redis\n    web --&gt;|State Management| redis        </code></pre> <p>:fire: TODO to configure a DMZ (firewall), we need some extra info: | Source IP | Destination IP | Protocol (HTTPS TCP/443 or DNS UDP/53 or ...) | |:---------:|:--------------:|:-----------------------------------------:| | 192.168.51|192.168.1.60    |HTTPS TCP/443                              |</p>"},{"location":"docs/httpsoffloading/#nginx","title":"NGINX","text":"<p>The proxy above may be implemented with different solutions:</p> <ul> <li>NGINX</li> <li>Traefik</li> <li>HAProxy</li> <li>Apache HTTP Server</li> <li>IIS</li> <li>Envoy Proxy</li> <li>Caddy</li> <li>F5</li> <li>...</li> </ul> <p>For this paricular setup, we will use NGINX.  </p> <p>The following NGINX configuration demonstrates a setup designed to facilitate secure HTTPS connections to a load-balanced backend comprised of multiple HTTPS API servers:</p> <pre><code>flowchart TD;        \n    RESTAPI1[REST Endpoint node1];\n    RESTAPI2[REST Endpoint node2];\n    RESTAPI3[REST Endpoint node2];\n\n    NGINX--load balances/HTTPS--&gt;RESTAPI1;\n    NGINX--load balances/HTTPS--&gt;RESTAPI2;\n    NGINX--load balances/HTTPS--&gt;RESTAPI3;\n\n    RESTAPI1--data protection--&gt;Redis;\n    RESTAPI2--data protection--&gt;Redis;\n    RESTAPI3--data protection--&gt;Redis;\n\n    Client--HTTPS--&gt;NGINX</code></pre> <p>The <code>NGINX</code> configuration for the following setup illustrates a setup for managing encrypted, load-balanced connections to a backend API cluster, with a focus on detailed logging, security with SSL, and flexibility to support large headers and optional WebSocket connections:  </p> <pre><code>http {\n    log_format upstreamlog '[$time_local] $remote_addr - $remote_user - $server_name $host to: $upstream_addr: $request $status upstream_response_time $upstream_response_time msec $msec request_time $request_time';\n\n    upstream https_api {\n        server 192.168.1.64:5000;\n        server 192.168.1.68:5000;\n        server 192.168.1.69:5000;\n    }\n\n    server {\n        listen 443 ssl;\n        listen [::]:443 ssl;\n        include snippets/self-signed.conf;\n        include snippets/ssl-params.conf;\n\n        client_header_buffer_size 4k;\n        large_client_header_buffers 4 8k; # Increase if JWT or other headers are large      \n\n        access_log /var/log/nginx/access.log upstreamlog;\n\n        # The location ^~ /api/ block captures all requests starting with /api/ and forwards them to the https_api upstream. The ^~ modifier gives this location precedence over regular expression locations that might also match.\n        location ^~ /api/ {\n            # The proxy_pass https://https_api; directive tells NGINX to forward the requests to the defined upstream group https_api. Ensure the scheme (https://) matches your upstream configuration. If your upstream servers are configured with HTTP, change the scheme accordingly.\n            proxy_pass https://https_api;\n\n            # Header adjustments are defined inside the /api/ location block to ensure they're applied only to the proxied requests.          \n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_set_header Authorization $http_authorization;\n\n            # Additional necessary proxy settings\n        }\n\n        # The location / block is modified to return a 404 error for any requests that don't match the /api/ path. This effectively drops traffic not intended for the API.        \n        location / {\n            return 404;\n        }\n\n      # Optional: Redirect the root path specifically if necessary\n      # location = / {\n      #     return 404;\n      # }\n\n      # Other possible headers you might want to set\n      # proxy_set_header Authorization \"\"; # If you need to reset the Authorization header\n\n      # WebSocket support (if needed)\n      # proxy_http_version 1.1;\n      # proxy_set_header Upgrade $http_upgrade;\n      # proxy_set_header Connection \"upgrade\";\n    }\n}\n</code></pre> <p>If we'd like to include the web servers in the diagram and configured as above, we get a diagram like:</p> <pre><code>flowchart TD;        \n    subgraph /\n        web1[Web Server node1]\n        web2[Web Server node2]    \n    end\n    subgraph /api\n        RESTAPI1[REST Endpoint node1];\n        RESTAPI2[REST Endpoint node2];\n        RESTAPI3[REST Endpoint node2];\n    end\n\n    NGINX--load balances/HTTPS--&gt;RESTAPI1;\n    NGINX--load balances/HTTPS--&gt;RESTAPI2;\n    NGINX--load balances/HTTPS--&gt;RESTAPI3;\n\n\n    NGINX--load balances/HTTPS--&gt;web1;\n    NGINX--load balances/HTTPS--&gt;web2;    \n\n\n    web1--data protection--&gt;Redis;\n    web2--data protection--&gt;Redis;   \n\n    RESTAPI1--data protection--&gt;Redis;\n    RESTAPI2--data protection--&gt;Redis;\n    RESTAPI3--data protection--&gt;Redis;\n\n    Client--HTTPS--&gt;NGINX</code></pre> <p>and an NGINX config:</p> <pre><code>http {\n    log_format upstreamlog '[$time_local] $remote_addr - $remote_user - $server_name $host to: $upstream_addr: $request $status upstream_response_time $upstream_response_time msec $msec request_time $request_time';\n\n    upstream https_api {\n        server 192.168.1.64:5000;\n        server 192.168.1.68:5000;\n        server 192.168.1.69:5000;\n    }\n\n    upstream web_api {\n        server 192.168.1.60:5000;\n        server 192.168.1.61:5000;\n    }\n\n    server {\n        listen 443 ssl;\n        listen [::]:443 ssl;\n        # The server_name directive specifies cars.be as the domain this server block will respond to, also for /api/!\n        server_name cars.be;\n\n        include snippets/self-signed.conf;\n        include snippets/ssl-params.conf;\n\n        client_header_buffer_size 4k;\n        large_client_header_buffers 4 8k; # Increase if JWT or other headers are large      \n\n        access_log /var/log/nginx/access.log upstreamlog;\n\n        # Serve the website for cars.be except for paths under /api/\n        location / {\n            proxy_pass https://web_api/\n        }\n\n        # The location ^~ /api/ block captures all requests starting with /api/ and forwards them to the https_api upstream. The ^~ modifier gives this location precedence over regular expression locations that might also match.\n        location ^~ /api/ {\n            # The proxy_pass https://https_api; directive tells NGINX to forward the requests to the defined upstream group https_api. Ensure the scheme (https://) matches your upstream configuration. If your upstream servers are configured with HTTP, change the scheme accordingly.\n            proxy_pass https://https_api;\n\n            # Header adjustments are defined inside the /api/ location block to ensure they're applied only to the proxied requests.          \n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_set_header Authorization $http_authorization;\n\n            # Additional necessary proxy settings\n        }\n\n        # The location / block is modified to return a 404 error for any requests that don't match the /api/ path. This effectively drops traffic not intended for the API.        \n        location / {\n            return 404;\n        }\n\n      # Optional: Redirect the root path specifically if necessary\n      # location = / {\n      #     return 404;\n      # }\n\n      # Other possible headers you might want to set\n      # proxy_set_header Authorization \"\"; # If you need to reset the Authorization header\n\n      # WebSocket support (if needed)\n      # proxy_http_version 1.1;\n      # proxy_set_header Upgrade $http_upgrade;\n      # proxy_set_header Connection \"upgrade\";\n    }\n}\n</code></pre> <p>The configuration above now serves a website for requests to cars.be on port 443 with HTTPS, except for HTTPS requests to cars.be/api/, which are proxied to the upstream https_api.  </p> <p>As mentioned, the public certificate may be delivered by Let's Encrypt. If we'd like to automatically request/renew certificates, Let's Encrypt will need to check that we are the owners of our domain (cars.be in this example). Let's Encrypt executes such a test by requesting a special file from the http://domain/.well-known/acme-challenge directory. <code>certbot</code> Is the most common client for Let's Enrypt to generate this special file.  </p> <pre><code>http {\n    log_format upstreamlog '[$time_local] $remote_addr - $remote_user - $server_name $host to: $upstream_addr: $request $status upstream_response_time $upstream_response_time msec $msec request_time $request_time';\n\n    upstream https_api {\n        server 192.168.1.64:5000;\n        server 192.168.1.68:5000;\n        server 192.168.1.69:5000;\n    }\n\n    upstream web_api {\n        server 192.168.1.60:5000;\n        server 192.168.1.61:5000;\n    }\n\n    server {\n        listen 80;\n        server_name cars.be;\n\n        location /.well-known/acme-challenge/ {\n            root /var/www/letsencrypt;\n            allow all;\n        }\n\n        location / {\n            return 301 https://$host$request_uri;\n        }\n    }\n\n    server {\n        listen 443 ssl;\n        listen [::]:443 ssl;\n        # The server_name directive specifies cars.be as the domain this server block will respond to, also for /api/!\n        server_name cars.be;\n\n        include snippets/self-signed.conf;\n        include snippets/ssl-params.conf;\n\n        client_header_buffer_size 4k;\n        large_client_header_buffers 4 8k; # Increase if JWT or other headers are large      \n\n        access_log /var/log/nginx/access.log upstreamlog;\n\n        # Serve the website for cars.be except for paths under /api/\n        location / {\n            proxy_pass https://web_api/;\n        }\n\n        # The location ^~ /api/ block captures all requests starting with /api/ and forwards them to the https_api upstream. The ^~ modifier gives this location precedence over regular expression locations that might also match.\n        location ^~ /api/ {\n            # The proxy_pass https://https_api; directive tells NGINX to forward the requests to the defined upstream group https_api. Ensure the scheme (https://) matches your upstream configuration. If your upstream servers are configured with HTTP, change the scheme accordingly.\n            proxy_pass https://https_api;\n\n            # Header adjustments are defined inside the /api/ location block to ensure they're applied only to the proxied requests.          \n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_set_header Authorization $http_authorization;\n\n            # Additional necessary proxy settings\n        }\n\n      # Other possible headers you might want to set\n      # proxy_set_header Authorization \"\"; # If you need to reset the Authorization header\n\n      # WebSocket support (if needed)\n      # proxy_http_version 1.1;\n      # proxy_set_header Upgrade $http_upgrade;\n      # proxy_set_header Connection \"upgrade\";\n    }\n}\n</code></pre> <p>In the NGINX configuration examples above, the routing of traffic is based on URLs, if we route traffic based on server names we get the following representation:</p> <pre><code>flowchart TB\n    client([Client]) --&gt;|HTTPS request| nginx[NGINX]\n    nginx --&gt;|Decides based on identity.cars.be| identity[NGINX server block &lt;/br&gt; for Authentication]\n    nginx --&gt;|Decides based on abpapi.cars.be| abpapi[NGINX server block &lt;/br&gt; for REST]\n    nginx --&gt;|Decides based on cars.be| web[NGINX server block &lt;/br&gt; for Web]\n\n    subgraph identityZ [ ]\n        identity --&gt; identity_api([Upstream identity_api])\n        identity_api --&gt;|HTTPS traffic| Authentication1\n        identity_api --&gt;|HTTPS traffic| Authentication2\n    end\n\n    subgraph abpapiZ [ ]\n        abpapi --&gt; abp_api([Upstream abp_api])\n        abp_api --&gt;|HTTPS traffic| REST1\n        abp_api --&gt;|HTTPS traffic| REST2\n    end\n\n    subgraph webZ [ ]\n        web --&gt; web_api([Upstream web_api])\n        web_api --&gt;|HTTPS traffic| WEB1\n        web_api --&gt;|HTTPS traffic| WEB2\n    end    </code></pre> <p>For our particular setup, this becomes:</p> <pre><code>events {\n}\n\nhttp {\n\n    client_header_buffer_size 4k;\n    #large_client_header_buffers 4 8k; # Increase if JWT or other headers are large      \n    proxy_buffer_size   128k;\n    proxy_buffers   4 256k;\n    proxy_busy_buffers_size   256k;\n    large_client_header_buffers 4 16k;\n\n    log_format upstreamlog '[$time_local] $remote_addr - $remote_user - $server_name $host to: $upstream_addr: $request $status upstream_response_time $upstream_response_time msec $msec request_time $request_time';\n\n    upstream abp_api {\n        {% for host in groups['http_api'] %}\n        server {{ host }}:{{ hostvars[host]['service_port'] }};\n        {% endfor %}\n    }\n\n    upstream web_api {        \n        {% for host in groups['webserver'] %}\n        server {{ host }}:{{ hostvars[host]['service_port'] }};\n        {% endfor %}\n    }\n\n    upstream identity_api {        \n        {% for host in groups['identity_server'] %}\n        server {{ host }}:{{ hostvars[host]['service_port'] }};\n        {% endfor %}\n    }\n\n    server {\n        listen 443 ssl;\n        listen [::]:443 ssl;\n        # The server_name directive specifies cars.be as the domain this server block will respond to, also for /api/!\n        server_name identity.{{ project_domain }};\n\n        include snippets/self-signed.conf;\n        include snippets/ssl-params.conf;\n\n        access_log /var/log/nginx/access.log upstreamlog;\n\n        # Serve the website for identity.cars.be\n        location / {\n            proxy_pass https://identity_api/;\n\n            # Header adjustments are defined inside the /api/ location block to ensure they're applied only to the proxied requests.          \n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_set_header Authorization $http_authorization;\n\n            # Additional necessary proxy settings\n        }    \n    }\n\n    server {\n        listen 443 ssl;\n        listen [::]:443 ssl;\n        # The server_name directive specifies cars.be as the domain this server block will respond to, also for /api/!\n        server_name abpapi.{{ project_domain }};\n\n        include snippets/self-signed.conf;\n        include snippets/ssl-params.conf;\n\n        access_log /var/log/nginx/access.log upstreamlog;\n\n        # Serve the website for identity.cars.be\n        location / {\n            proxy_pass https://abp_api/;\n\n            # Header adjustments are defined inside the /api/ location block to ensure they're applied only to the proxied requests.          \n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_set_header Authorization $http_authorization;\n\n            # Additional necessary proxy settings\n        }    \n    }\n\n    server {\n        listen 443 ssl;\n        listen [::]:443 ssl;\n        # The server_name directive specifies cars.be as the domain this server block will respond to, also for /api/!\n        server_name {{ project_domain }};\n\n        include snippets/self-signed.conf;\n        include snippets/ssl-params.conf;\n\n        access_log /var/log/nginx/access.log upstreamlog;\n\n        # Serve the website for cars.be except for paths under /api/\n        location / {\n            proxy_pass https://web_api/;\n\n            # Header adjustments are defined inside the /api/ location block to ensure they're applied only to the proxied requests.          \n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_set_header Authorization $http_authorization;\n\n            # Additional necessary proxy settings\n        }        \n\n      # Other possible headers you might want to set\n      # proxy_set_header Authorization \"\"; # If you need to reset the Authorization header\n\n      # WebSocket support (if needed)\n      proxy_http_version 1.1;\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection \"upgrade\";\n    }\n}\n</code></pre> <p>As routing based on hostnames has less impact on the web server and REST endpoint source code (no constraints on URLS), routing based on hostnames is the preferred solution in this project.  </p>"},{"location":"docs/httpsoffloading/#log-format","title":"Log format","text":"<p>To capture detailed information about requests and responses, we define a custom logging format: the <code>log_format</code> directive specifies a custom format named <code>upstreamlog</code>, which includes timestamps, client IP, the requested server name and host, the upstream server address that handled the request, the request details, status code, response times, and request processing times. This detailed logging is crucial for debugging and monitoring the performance of the upstream servers, in particular inspecting load balancing as the log will display the IP address of the upstream REST API node.</p>"},{"location":"docs/httpsoffloading/#load-balancing","title":"Load balancing","text":"<p>Distribute incoming traffic across multiple backend servers: the upstream block named <code>http_api</code> defines a cluster of servers (with specified IP addresses and port numbers) that requests will be load balanced across. This setup increases the application's scalability and reliability by distributing the load and providing redundancy.</p>"},{"location":"docs/httpsoffloading/#https-configuration","title":"HTTPS configuration","text":"<p>We configure NGINX to serve HTTPS traffic and to use specific SSL parameters and certificates.</p> <ul> <li>The listen 443 ssl directives instruct NGINX to listen for incoming connections on port 443 with SSL encryption, including IPv6 connections ([::]:443 ssl).</li> <li>The include directives incorporate additional SSL configurations and self-signed certificate details from external files (self-signed.conf and ssl-params.conf), modularizing the SSL setup for easier management.</li> <li>client_header_buffer_size And large_client_header_buffers directives adjust the buffer sizes for client request headers, accommodating potentially large JWT tokens or other large headers.</li> </ul> <p>Note the HTTPS offloading and re-encryption in the configuration above which involves the NGINX server acting as a termination point for incoming HTTPS connections from clients. NGINX decrypts these connections, inspects the traffic, and then re-encrypts the traffic before forwarding it to the backend servers over HTTPS using separate, internal SSL certificates. This process allows for secure communication both externally with clients and internally within the data center:</p> <ul> <li>Decryption of Incoming HTTPS Requests: NGINX uses public-facing SSL certificates to decrypt data received over SSL/TLS from clients. This is the offloading part.</li> <li>Inspection and Processing: Once decrypted, NGINX can inspect, log, or manipulate the HTTP content as necessary. This step can involve modifying headers, applying access controls, or making routing decisions.</li> <li>Re-Encryption and Forwarding: NGINX then re-encrypts the requests using internal SSL certificates and forwards them to the backend servers over HTTPS. This ensures that traffic remains secure as it traverses the internal network.</li> <li>Secure Internal Communication: The backend servers, configured with the internal certificates, decrypt the re-encrypted requests, process them, and send back the responses. NGINX then encrypts the responses again (if necessary) before sending them back to the clients.</li> </ul> <p>HTTPS offloading and re-encryption has the following benefits:</p> <ul> <li>Enhanced Security Across the Board: This setup ensures end-to-end encryption of data, maintaining security from the client to the NGINX server and from the NGINX server to the backend servers. Using internal certificates for the data center adds an extra layer of security within the internal network.</li> <li>Centralized Public SSL Management: Public-facing SSL certificate management remains centralized at the NGINX server, simplifying the administration of public encryption keys and certificates.</li> <li>Flexible Trust and Security Policies: The separation of external and internal certificates allows for distinct trust domains and security policies. For example, stronger or different encryption standards can be applied internally compared to what is used for public internet traffic.</li> <li>Inspection and Added Controls: Decrypting the traffic at the NGINX layer allows for detailed inspection and the application of additional security controls before re-encrypting and sending it to backend services. This can be crucial for compliance with security policies, logging, or performing deep packet inspection for threat detection.</li> <li>Performance and Scalability: While the NGINX server handles the computational overhead of encrypting and decrypting traffic twice, it offloads backend servers from doing so. This can be optimized by using hardware that accelerates SSL processing. Furthermore, it allows backend servers to be scaled out without the need to manage public SSL certificates on each of them.</li> <li>Secure Internal Traffic: Using HTTPS internally protects against potential threats lurking within the data center, ensuring that sensitive data is encrypted in transit even within the network's trusted boundaries.</li> </ul> <p>In summary, employing HTTPS offloading and re-encryption with NGINX provides robust security by ensuring encrypted traffic both externally and internally while centralizing certificate management and offering the ability to inspect and manipulate HTTP traffic. This configuration enhances the overall security posture and flexibility of managing traffic flows within distributed application architectures.</p>"},{"location":"docs/httpsoffloading/#access-logging","title":"Access logging","text":"<p>In orde to use a custom-defined log file path and format, the <code>access_log</code> directive specifies the path to the access log file and instructs NGINX to use the upstreamlog format (<code>log_format upstreamlog '[$ti...</code>) for logging, ensuring that detailed information about each request and its handling is logged for future analysis.</p>"},{"location":"docs/httpsoffloading/#location-block-and-proxy-settings","title":"Location block and proxy settings","text":"<p>Request routing and proxying behavior for the root path is defined with:</p> <ul> <li>The location / block matches all requests and uses the proxy_pass directive to forward them to the http_api upstream cluster, enabling load balancing.</li> <li>The proxy_set_header directives modify request headers to include the original host, the real client IP address, and the protocol used, ensuring that the proxied requests contain necessary information for backend services to process them correctly.</li> <li>The configuration optionally supports forwarding the Authorization header, preserving JWT or other authentication tokens needed by the backend services.</li> </ul>"},{"location":"docs/httpsoffloading/#optional-websocket-support","title":"Optional websocket support","text":"<p>This part of the configuration provides the necessary headers to support WebSocket connections if needed:</p> <ul> <li>The commented-out directives (<code>proxy_http_version</code>, <code>proxy_set_header Upgrade</code>, and <code>proxy_set_header Connection</code>) are set up to enable WebSocket support, ensuring that NGINX can proxy WebSocket connections correctly. These lines can be uncommented and adjusted as needed based on the application's requirements.</li> </ul>"},{"location":"docs/proxmox/","title":"Proxmox","text":""},{"location":"docs/proxmox/#proxmox-vm-setup","title":"Proxmox VM setup","text":"<p>As far as I know, Ansible is not necessarily the tool of choice to setup VMs on cloud/datacenter solutions like Proxmox, etc. Nevertheless, as an excercise, I've chosen to setup my Proxmox VMs with Ansible.  </p> <p>Ansible Uses an <code>inventory</code> to define the machines (ip addresses) and machine roles to install/setup your software infrastructure. As we start with a clean install (no existing VMs), it is rather difficult to define an <code>inventory</code>.  </p> <p>Therefore, we start with a play proxmox which uses role and vars file defining the inventory, for example:</p> <pre><code>proxmox_vms:\n - ip: 192.168.1.60\n   id: 600\n   group:\n    - \"webserver\"\n - ip: 192.168.1.61\n   id: 601\n   group:\n    - \"redis\"\n - ip: 192.168.1.62\n   id: 602\n   group:\n    - \"database\"\n</code></pre> <p>On the other hand, it is probably wise to structure/group ip addresses and vm ids under <code>group:</code> which is more aligned with an Ansible inventory (and easier to maintain). :fire: That's for another version.  </p> <p>After running the proxmox play, we can use the initialized VMs as defined in the vars file as the basis for the Ansible <code>\u00ecnventory</code>.  </p> <p>Running the proxmox play is detailed in Uploading templates and Installing VMs on Proxmox.  </p> <p>:fire: The hosts file contains the <code>inventory</code> for the other Ansible plays. :fire: At this moment, the hosts file is not generated from the proxmox play, it is probably a good idea to change that in future versions.  </p>"},{"location":"docs/proxmox/#uploading-proxmox-vms-templates","title":"Uploading Proxmox VM(s) templates","text":"<pre><code>ansible-playbook proxmox.yml -K --tags=vm_upload -vvv\n</code></pre> <p>:fire: If this tag fails, you may have to upgrade your community.general modules, see issue:</p> <pre><code>ansible-galaxy collection install community.general\n</code></pre> <p>you should have at least version 7.2.1</p> <pre><code>ansible-galaxy collection list\n</code></pre> <p>and look for community.general.</p> <p>:fire: You probably have two installs of community.general, if you'd like to run the system-wide install:</p> <pre><code>sudo ansible-galaxy collection install community.general\n</code></pre>"},{"location":"docs/proxmox/#installing-proxmox-vms","title":"Installing Proxmox VM(s)","text":"<pre><code>ansible-playbook proxmox.yml -K --tags=vm_init -vvv\n</code></pre>"},{"location":"docs/proxmox/#addding-proxmox-vms-to-known_hosts","title":"Addding Proxmox VM(s) to known_hosts","text":"<p>If you removed existing VMs/Containers or are building new VMs/Containers, you should remove the old entries in <code>known_hosts</code> and add new entries:</p> <pre><code>ANSIBLE_HOST_KEY_CHECKING=false ansible-playbook knowhosts-setup.yml -i hosts -K -vvv\n</code></pre> <p>When you observe an additional line being added to your known_hosts file after connecting to a server via SSH, even after manually adding the server's fingerprint with ssh-keyscan or with <code>ansible-playbook knowhosts-setup.yml</code>, it typically relates to how SSH handles and verifies the identity of the servers it connects to. There are a few reasons why this might happen:</p> <ol> <li> <p>Different Key Types If you initially add the server's <code>ED25519</code> key fingerprint to your known_hosts using ssh-keyscan -H -t ed25519 ip_address, but the server is also configured to use another type of SSH key (e.g., RSA, ECDSA), the SSH client might add the fingerprint of this additional key to the known_hosts file upon the first connection. This occurs because your initial scan and add operation only included the ED25519 key, and upon connection, SSH automatically adds any other keys presented by the server that weren't already in known_hosts.</p> </li> <li> <p>Hostname and IP Address Entries Another common reason is the difference in how you reference the server in your ssh command versus what was initially scanned. For instance, if you scanned the IP address and then used the hostname (or vice versa) to connect, SSH treats these as separate entries. SSH distinguishes between IP addresses and hostnames because they can technically present different keys (consider virtual hosts or shared IP scenarios). As a result, SSH might add a new line for the same server under its different identifier (IP or hostname).</p> </li> <li> <p>SSH Configuration and Aliases Your SSH client's configuration might influence how known_hosts is managed. For example, if you use an SSH config file (~/.ssh/config) with aliases or specific host entries that define hostname patterns or specific key types, your SSH client might treat connections that match different patterns as distinct, even if they ultimately resolve to the same server.</p> </li> <li> <p>Port Forwarding or Jump Hosts If your connection involves port forwarding or using a jump host (also known as a bastion host), the SSH client may add entries for these intermediate steps. This is more likely in complex networking setups where direct connections to the target server are not possible without going through intermediary servers.</p> </li> </ol> <p>Troubleshooting Tips:</p> <ul> <li>Review the known_hosts file: Compare the entries before and after the connection. Look for differences in the key types, hostnames/IP addresses, or additional details that might explain the new entry.</li> <li>Use verbose mode with SSH: Connecting with <code>ssh -v root@ip_address</code> can provide detailed logs that explain what keys are being checked, offered, and added to known_hosts. This might give you a clearer picture of why the additional line is added.</li> <li>Check SSH client configuration: Review your SSH client's configuration file (if you have one) for any settings that might affect how known_hosts entries are managed or how connections are established.</li> </ul> <p>Understanding the exact reason requires examining the specifics of your SSH setup, the server configuration, and the entries in known_hosts.</p> <p>For our particular setup, we can edit the server config <code>sshd_config</code> file on the server we connect to to enforce the usage of <code>ED25519</code> keys:</p> <pre><code>#HostKey /etc/ssh/ssh_host_rsa_key\n#HostKey /etc/ssh/ssh_host_ecdsa_key\nHostKey /etc/ssh/ssh_host_ed25519_key\nKexAlgorithms curve25519-sha256,curve25519-sha256@libssh.org\nPubkeyAcceptedKeyTypes ssh-ed25519\n</code></pre> <p>in addition, make sure that folder <code>/run/sshd</code> exists on the server:</p> <pre><code>mkdir /run/sshd\nchmod 0755 /run/sshd\nsystemctl restart sshd\n</code></pre> <p>Clients, however, should now support <code>ED25519</code>, i.e. use:</p> <pre><code>ssh-keygen -t ed25519 -C you@cars.be\n</code></pre> <p>If you now connect with:</p> <pre><code>ssh -v root@192.168.1.68\n</code></pre> <p>you will notice in the output that ssh will attempt different keys, for example:</p> <pre><code>debug1: Will attempt key: /home/you/.ssh/id_rsa RSA SHA256:....\ndebug1: Will attempt key: /home/you/.ssh/id_ecdsa\ndebug1: Will attempt key: /home/you/.ssh/id_ecdsa_sk\ndebug1: Will attempt key: /home/you/.ssh/id_ed25519 ED25519 SHA256:....\ndebug1: Will attempt key: /home/you/.ssh/id_ed25519_sk\ndebug1: Will attempt key: /home/you/.ssh/id_xmss\ndebug1: Will attempt key: /home/you/.ssh/id_dsa\n</code></pre> <p>To conclude, the server decides about the security of the keys used by the client, therefore, we will configure all sshd daemons for <code>ED25519</code>:  </p> <pre><code>ansible-playbook sshdserver_setup.yml -i hosts --user root -K -vvv\n</code></pre> <p>:fire: User root is specified with <code>--user root</code>: the hosts we connect to have the <code>ED25519</code> key under <code>/root/.ssh/authorized_keys</code>, so we have to connect as <code>root</code> to use this key.  </p>"},{"location":"docs/proxmox/#stopping-proxmox-vms","title":"Stopping Proxmox VM(s)","text":"<pre><code>ansible-playbook proxmox.yml -K --tags=vm_stop -vvv\n</code></pre>"},{"location":"docs/proxmox/#starting-proxmox-vms","title":"Starting Proxmox VM(s)","text":"<pre><code>ansible-playbook proxmox.yml -K --tags=vm_start -vvv\n</code></pre> <p>if necessary, you can restart with</p> <pre><code>ansible-playbook proxmox.yml -K --tags=vm_restart -vvv\n</code></pre>"},{"location":"docs/proxmox/#removing-proxmox-vms","title":"Removing Proxmox VM(s)","text":"<p>Before removing a Proxmox VM, first stop the Proxmox VM, so execute the stopping proxmox command or add tag vm_stop before the vm_remove tag in the following command:</p> <pre><code>ansible-playbook proxmox.yml -K --tags=vm_stop,vm_remove -vvv\n</code></pre>"},{"location":"docs/proxy/","title":"Proxy","text":""},{"location":"docs/proxy/#install-nginx-proxy","title":"Install NGINX Proxy","text":"<pre><code>ansible-playbook nginx.yml -i hosts -K -vvv\n</code></pre>"},{"location":"docs/redis/","title":"Redis","text":"<p>Cross-Site Request Forgery (CSRF) is a security threat where an attacker tricks a user into executing unwanted actions on a web application in which they're authenticated. If the victim is a regular user, a successful CSRF attack can force them to perform state-changing requests like transferring funds, changing their email address, and so forth. If the victim has an administrative account, CSRF can compromise the entire web application. CSRF exploits the trust that a site has in the user's browser, and unlike Cross-Site Scripting (XSS), which exploits the trust a user has in a particular site, CSRF exploits the trust that a site has in the user's browser.</p> <p>In a distributed web application architecture, particularly one that scales horizontally as in this setup, requests from the same user can be routed to different servers across multiple requests due to load balancing. This poses a challenge for CSRF protection mechanisms that rely on keeping track of state, such as synchronizer tokens or double submit cookies, because the server handling a subsequent request might not have access to the tokens generated by another server on a previous request.</p> <p>REDIS, as a fast, in-memory data store offers a solution for storing CSRF protection keys in such a distributed setup. </p> <p>In the drawing above, REDIS was added explicitely to stress the impact of the distributed nature and horizontal scalability of the setup, i.e. its non-trivial impact on CSRF. More details are available on the cars.be repo.</p>"},{"location":"docs/redis/#install-redis","title":"Install REDIS","text":"<pre><code>ansible-playbook redis.yml -i hosts -K -vvv\n</code></pre> <p>:fire: Note the <code>hosts</code> file, at the moment, only ip address are used in the host file. In an ideal case, hostnames as setup on Proxmox should be used grouped by labels redis, webserver, etc.</p> <p>:fire: If you get an error like</p> <pre><code> \"msg\": \"Unable to start service redis-server: Job for redis-server.service failed because the control process exited with error code.\\nSee \\\"systemctl status redis-server.service\\\" and \\\"journalctl -xe\\\" for details.\\n\"\n</code></pre> <p>after running the redis playbook, you probably forgot to <code>source .env</code>.  </p>"},{"location":"docs/references/","title":"References","text":"<ul> <li>Playbooks directory</li> <li>Mardown all-in-one</li> </ul>"},{"location":"docs/repositoryintroduction/","title":"Repository Introduction","text":"<p>Welcome to our GitHub repository where you'll find Ansible scripts written to facilitate the development of web applications by mirroring a typical production environment. This approach might address common challenges where hardware or time constraints make it difficult to set up a test or acceptance environment that accurately reflects various production settings.</p>"},{"location":"docs/repositoryintroduction/#purpose-of-this-repository","title":"Purpose of This Repository","text":"<p>This repository hosts Ansible scripts that allow you to deploy a complex web application environment either locally on your development machine, on-premises hardware, or in a cloud setup. By leveraging these scripts, developers may accelerate the setup process and ensure consistency across different environments, from development to production.</p>"},{"location":"docs/repositoryintroduction/#environment-roles-defined","title":"Environment Roles Defined","text":"<p>The provided Ansible scripts are prepared to configure the following roles essential for a full-featured web application infrastructure:</p> <ul> <li>Web Server: Hosts the web application interface.</li> <li>Cache: Improves response time and reduces load on the database by storing recently accessed data.</li> <li>Database: Manages data storage and retrieval.</li> <li>Identity Server: Handles authentication and authorization services.</li> <li>REST API endpoints: Facilitates communication between the client apps and server.</li> <li>Load Balancer: Distributes incoming network traffic across multiple servers.</li> <li>Certificate Authority: Issues SSL/TLS certificates for secured communications.</li> <li>HTTPS offloading: Relieves backend application servers of the processing load associated with encrypting and decrypting public HTTPS traffic.</li> <li>Prometheus monitoring: Collects and stores metrics as time-series data.</li> <li>Grafana dashboards: Provides visualization of the metrics collected by Prometheus.</li> </ul>"},{"location":"docs/restapi/","title":"REST API","text":""},{"location":"docs/restapi/#install-rest-api","title":"Install REST API","text":"<pre><code>ansible-playbook abprestapi.yml -i hosts -K -vvv\n</code></pre> <p>The REST API contains a custom metric and controller, open the REST API interface (<code>Swagger</code>) and call <code>/api/car/increment</code> to increment the custom metric. If you'd like to monitor this metric in real-time, SSH into the HTTP server and run:  </p> <pre><code>#if nessary, install dotnet-counters\n#dotnet tool install --global dotnet-counters\ndotnet-counters monitor --name Be.Cars.HttpApi.Host --counters Be.Cars.Metrics.CustomMetrics\n</code></pre>"},{"location":"docs/setupinstructions/","title":"Setup Instructions","text":""},{"location":"docs/setupinstructions/#prerequisites","title":"Prerequisites","text":"<p>Before running the Ansible playbooks, ensure that you have the following:</p> <ul> <li>Ansible installed on your local machine or a control node.</li> <li>Sufficient privileges to manage the systems designated in your inventory.</li> <li>Machines (VMs, containers, or bare metal) defined in your hosts file and accessible over your network.</li> </ul>"},{"location":"docs/setupinstructions/#running-the-playbooks","title":"Running the Playbooks","text":"<p>To deploy the roles listed above, follow these steps:</p> <ol> <li> <p>Navigate to your local clone of this repository:    Open a terminal and ensure you are in the repository directory.</p> </li> <li> <p>Setup your inventory:    Ensure that your <code>hosts</code> file in the <code>./playbooks</code> directory is updated to reflect the infrastructure where you want to deploy the roles.</p> </li> <li> <p>Configure role variables    Default variables are provided for all roles in <code>role_folder/defaults/vars.yml</code> and initialized by reading environment variables. Example environment variables are available.</p> </li> <li> <p>Execute the Ansible Playbook:    Run the following command to start the deployment process. This command will prompt you for your sudo password for the hosts defined in the inventory due to the <code>-K</code> flag and provide detailed output with the <code>-vvv</code> verbosity level.</p> </li> </ol> <pre><code>ansible-playbook site.yml -K -i hosts -vvv\n</code></pre>"},{"location":"docs/setupinstructions/#expected-outcome","title":"Expected Outcome","text":"<p>After executing the playbook, all specified roles should be successfully configured and running across your specified machines. You can verify the status of each role through the corresponding management interfaces or by checking the services directly on each machine.</p>"},{"location":"docs/setupinstructions/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during the setup, consider the following troubleshooting steps:</p> <ul> <li>Verify network connectivity between your control node and the target machines.</li> <li>Check the permissions and authentication details for the accounts used.</li> <li>Review the verbose output provided by the Ansible execution for errors or warnings.</li> </ul>"},{"location":"docs/todo/","title":"TODO","text":"<ul> <li>move proxmox playbook to a different \"folder\"</li> <li>generate the Ansible <code>inventory</code> file or use an <code>inventory</code> file as a vars.yml file to setup VMs (if possible)</li> <li>integrate prometheus alerts or grafana alerts or ...</li> <li>enable tcpdump in a container https://netsplit.uk/posts/2022/10/19/remote_ovh_lab/</li> <li>pfSense <code>pfctl -d</code> and <code>pfctl -e</code> to disable/enable the packet filter</li> <li>add DNS</li> <li>remove cert generation outside of ca/caclient roles</li> </ul>"},{"location":"docs/webserver/","title":"Web Server","text":""},{"location":"docs/webserver/#install-the-blazor-application","title":"Install the Blazor Application","text":"<p>For this project, we will host the application (Blazor application) on Kestrel. <code>Kestrel</code> is automatically included by publishing the application.</p> <pre><code>ansible-playbook webserver.yml -i hosts -K -vvv\n</code></pre> <p>:fire: Install the web server after installing the database: while installing the web server, the DB migration project will run and initialize the database.  </p>"},{"location":"playbooks/roles/hostr.abprestapi/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"playbooks/roles/hostr.abprestapi/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"playbooks/roles/hostr.abprestapi/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"playbooks/roles/hostr.abprestapi/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"playbooks/roles/hostr.abprestapi/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"playbooks/roles/hostr.abprestapi/#license","title":"License","text":"<p>BSD</p>"},{"location":"playbooks/roles/hostr.abprestapi/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"playbooks/roles/hostr.authentication/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"playbooks/roles/hostr.authentication/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"playbooks/roles/hostr.authentication/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"playbooks/roles/hostr.authentication/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"playbooks/roles/hostr.authentication/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"playbooks/roles/hostr.authentication/#license","title":"License","text":"<p>BSD</p>"},{"location":"playbooks/roles/hostr.authentication/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"playbooks/roles/hostr.ca/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"playbooks/roles/hostr.ca/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"playbooks/roles/hostr.ca/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"playbooks/roles/hostr.ca/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"playbooks/roles/hostr.ca/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"playbooks/roles/hostr.ca/#license","title":"License","text":"<p>BSD</p>"},{"location":"playbooks/roles/hostr.ca/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"playbooks/roles/hostr.caclient/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"playbooks/roles/hostr.caclient/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"playbooks/roles/hostr.caclient/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"playbooks/roles/hostr.caclient/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"playbooks/roles/hostr.caclient/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"playbooks/roles/hostr.caclient/#license","title":"License","text":"<p>BSD</p>"},{"location":"playbooks/roles/hostr.caclient/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"playbooks/roles/hostr.docker/","title":"hostr.docker","text":""},{"location":"playbooks/roles/hostr.docker/#overview","title":"Overview","text":"<p>This Ansible role installs and configures Docker on a target host. It ensures that Docker is installed, configured with appropriate settings, and that the service is enabled and running.</p>"},{"location":"playbooks/roles/hostr.docker/#requirements","title":"Requirements","text":"<p>This role requires the target host to be a supported Ubuntu distribution with access to the internet for package installations.</p>"},{"location":"playbooks/roles/hostr.docker/#role-variables","title":"Role Variables","text":"<p>The following variables are available for this role and can be customized to suit your needs. Default values are set in <code>defaults/main.yml</code>: </p> Variable Default Value Example Value Description <code>http_proxy</code> `` http://my_proxy:my_proxy_port/ HTTP proxy used by docker. <code>https_proxy</code> `` http://my_proxy:my_proxy_port/ HTTPS proxy used by docker. <code>no_proxy</code> `` localhost,127.0.0.0/8,you.local,registry.you.local Do not use proxy. <code>docker_default_address_pools_base</code> `` 172.17.0.0/16 The IP subnet to choose Docker container IP address from. <code>docker_default_address_pools_size</code> `` 20 The size of the IP subnet. <code>docker_log_driver</code> `` json-file The Docker log driver. <code>docker_log_opts_max_size</code> `` 200m The maximum size of a Docker log file <code>docker_log_opts_max_file</code> `` 5 The maximum number of log files <code>docker_users</code> `` - name: user1  pubkey: ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIID42JAWkOxmP1WjuVVPNLeqsEq3KiiSHlWALLd22u68 bart@engine27.be  - name: user2  pubkey: ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIID42JAWkOxmP1WjuVVPNLeqsEq3KiiSHlWALLd22u68 bart@engine27.be List of users to be added to the <code>docker</code> group."},{"location":"playbooks/roles/hostr.docker/#dependencies","title":"Dependencies","text":"<p>This role has no dependencies on other roles.</p>"},{"location":"playbooks/roles/hostr.docker/#example-playbook","title":"Example Playbook","text":"<p>Below is an example of how to use this role in a playbook:</p> <pre><code>- name: Install and configure Docker\n  hosts: all\n  become: true\n  roles:\n    - role: hostr.docker\n      vars:\n        docker_users:\n          - user1\n          - user2\n        docker_log_driver: json-file\n        docker_log_opts_max_size: 200m\n        docker_log_opts_max_file: 5\n</code></pre>"},{"location":"playbooks/roles/hostr.docker/#license","title":"License","text":"<p>MIT</p>"},{"location":"playbooks/roles/hostr.docker/#author-information","title":"Author Information","text":"<p>This role was created by Bart.</p>"},{"location":"playbooks/roles/hostr.docker/#todo","title":"TODO","text":"<ul> <li>check for empty variables</li> </ul>"},{"location":"playbooks/roles/hostr.docker/#known-issues","title":"Known Issues","text":"<ul> <li><code>sudo user1</code>: sudo: setrlimit(RLIMIT_CORE): Operation not permitted</li> </ul>"},{"location":"playbooks/roles/hostr.grafana/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"playbooks/roles/hostr.grafana/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"playbooks/roles/hostr.grafana/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"playbooks/roles/hostr.grafana/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"playbooks/roles/hostr.grafana/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"playbooks/roles/hostr.grafana/#license","title":"License","text":"<p>BSD</p>"},{"location":"playbooks/roles/hostr.grafana/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"playbooks/roles/hostr.knownexternalhosts/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"playbooks/roles/hostr.knownexternalhosts/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"playbooks/roles/hostr.knownexternalhosts/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"playbooks/roles/hostr.knownexternalhosts/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"playbooks/roles/hostr.knownexternalhosts/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"playbooks/roles/hostr.knownexternalhosts/#license","title":"License","text":"<p>BSD</p>"},{"location":"playbooks/roles/hostr.knownexternalhosts/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"playbooks/roles/hostr.knownvmhosts/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"playbooks/roles/hostr.knownvmhosts/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"playbooks/roles/hostr.knownvmhosts/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"playbooks/roles/hostr.knownvmhosts/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"playbooks/roles/hostr.knownvmhosts/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"playbooks/roles/hostr.knownvmhosts/#license","title":"License","text":"<p>BSD</p>"},{"location":"playbooks/roles/hostr.knownvmhosts/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"playbooks/roles/hostr.mssql/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"playbooks/roles/hostr.mssql/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"playbooks/roles/hostr.mssql/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"playbooks/roles/hostr.mssql/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"playbooks/roles/hostr.mssql/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"playbooks/roles/hostr.mssql/#license","title":"License","text":"<p>BSD</p>"},{"location":"playbooks/roles/hostr.mssql/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"playbooks/roles/hostr.nginx/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"playbooks/roles/hostr.nginx/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"playbooks/roles/hostr.nginx/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"playbooks/roles/hostr.nginx/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"playbooks/roles/hostr.nginx/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"playbooks/roles/hostr.nginx/#license","title":"License","text":"<p>BSD</p>"},{"location":"playbooks/roles/hostr.nginx/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"playbooks/roles/hostr.otelcollector/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"playbooks/roles/hostr.otelcollector/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"playbooks/roles/hostr.otelcollector/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"playbooks/roles/hostr.otelcollector/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"playbooks/roles/hostr.otelcollector/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"playbooks/roles/hostr.otelcollector/#license","title":"License","text":"<p>BSD</p>"},{"location":"playbooks/roles/hostr.otelcollector/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"playbooks/roles/hostr.prometheus/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"playbooks/roles/hostr.prometheus/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"playbooks/roles/hostr.prometheus/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"playbooks/roles/hostr.prometheus/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"playbooks/roles/hostr.prometheus/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"playbooks/roles/hostr.prometheus/#license","title":"License","text":"<p>BSD</p>"},{"location":"playbooks/roles/hostr.prometheus/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"playbooks/roles/hostr.proxmox/","title":"Role Name","text":"<p>Setup of VM(s) on Proxmox for the https://cars.be website.</p>"},{"location":"playbooks/roles/hostr.proxmox/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"playbooks/roles/hostr.proxmox/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"playbooks/roles/hostr.proxmox/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"playbooks/roles/hostr.proxmox/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"playbooks/roles/hostr.proxmox/#license","title":"License","text":"<p>BSD</p>"},{"location":"playbooks/roles/hostr.proxmox/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"playbooks/roles/hostr.redis/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"playbooks/roles/hostr.redis/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"playbooks/roles/hostr.redis/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"playbooks/roles/hostr.redis/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"playbooks/roles/hostr.redis/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"playbooks/roles/hostr.redis/#license","title":"License","text":"<p>BSD</p>"},{"location":"playbooks/roles/hostr.redis/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"playbooks/roles/hostr.sshd/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"playbooks/roles/hostr.sshd/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"playbooks/roles/hostr.sshd/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"playbooks/roles/hostr.sshd/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"playbooks/roles/hostr.sshd/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"playbooks/roles/hostr.sshd/#license","title":"License","text":"<p>BSD</p>"},{"location":"playbooks/roles/hostr.sshd/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"playbooks/roles/hostr.webserver/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"playbooks/roles/hostr.webserver/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"playbooks/roles/hostr.webserver/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"playbooks/roles/hostr.webserver/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"playbooks/roles/hostr.webserver/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"playbooks/roles/hostr.webserver/#license","title":"License","text":"<p>BSD</p>"},{"location":"playbooks/roles/hostr.webserver/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"playbooks/roles/hostr.wireguard/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"playbooks/roles/hostr.wireguard/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"playbooks/roles/hostr.wireguard/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"playbooks/roles/hostr.wireguard/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"playbooks/roles/hostr.wireguard/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"playbooks/roles/hostr.wireguard/#license","title":"License","text":"<p>BSD</p>"},{"location":"playbooks/roles/hostr.wireguard/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"}]}